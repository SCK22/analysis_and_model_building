{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Without-data-standardization\" data-toc-modified-id=\"Without-data-standardization-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Without data standardization</a></span></li><li><span><a href=\"#Model-definition-(Vanilla-MLP)\" data-toc-modified-id=\"Model-definition-(Vanilla-MLP)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model definition (Vanilla MLP)</a></span></li></ul></li><li><span><a href=\"#Looking-at-the-model-structure\" data-toc-modified-id=\"Looking-at-the-model-structure-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Looking at the model structure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-performance\" data-toc-modified-id=\"Model-performance-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Model performance</a></span></li></ul></li><li><span><a href=\"#With-data-standardization\" data-toc-modified-id=\"With-data-standardization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>With data standardization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-performance\" data-toc-modified-id=\"Model-performance-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Model performance</a></span></li></ul></li><li><span><a href=\"#MLP-with-tweaks\" data-toc-modified-id=\"MLP-with-tweaks-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>MLP with tweaks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-performance\" data-toc-modified-id=\"Model-performance-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Model performance</a></span></li><li><span><a href=\"#Training-metrics-changes\" data-toc-modified-id=\"Training-metrics-changes-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Training metrics changes</a></span></li></ul></li><li><span><a href=\"#Deep-MLP-with-random-grid-search\" data-toc-modified-id=\"Deep-MLP-with-random-grid-search-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Deep MLP with random grid search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-the-grid-space\" data-toc-modified-id=\"Defining-the-grid-space-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Defining the grid space</a></span></li><li><span><a href=\"#using-the-best-model-for-evaluation\" data-toc-modified-id=\"using-the-best-model-for-evaluation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>using the best model for evaluation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"c:\\\\users\\\\chait\\\\anaconda3\\\\lib\\\\site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categorical_for_dl(x):\n",
    "    if x ==\"ENGAGED\":return 1 \n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"model_data/X_train.csv\")\n",
    "y_train = pd.read_csv(\"model_data/y_train.csv\")\n",
    "X_validation = pd.read_csv(\"model_data/X_validation.csv\")\n",
    "y_validation = pd.read_csv(\"model_data/y_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "\n",
    "x_train = X_train.values\n",
    "x_validation = X_validation.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12835 train samples\n",
      "5502 test samples\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape[0], 'train samples')\n",
    "\n",
    "print(x_validation.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12835"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.Series(y_train.is_user_engaged).apply(lambda x : convert_categorical_for_dl(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation = pd.Series(y_validation.is_user_engaged).apply(lambda x : convert_categorical_for_dl(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_validation = tensorflow.keras.utils.to_categorical(y_validation, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition (Vanilla MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               20992     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 284,674\n",
      "Trainable params: 284,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               20992     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 284,674\n",
      "Trainable params: 284,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12835, 40)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12835, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['binary_crossentropy'],\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy', recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12835 samples, validate on 5502 samples\n",
      "Epoch 1/20\n",
      "12835/12835 [==============================] - 4s 327us/sample - loss: 1.2267 - accuracy: 0.8644 - recall: 0.8652 - val_loss: 1.2193 - val_accuracy: 0.8753 - val_recall: 0.8753\n",
      "Epoch 2/20\n",
      "12835/12835 [==============================] - 1s 103us/sample - loss: 0.9691 - accuracy: 0.8778 - recall: 0.8772 - val_loss: 0.9713 - val_accuracy: 0.8566 - val_recall: 0.8566\n",
      "Epoch 3/20\n",
      "12835/12835 [==============================] - 1s 111us/sample - loss: 0.5997 - accuracy: 0.8811 - recall: 0.8805 - val_loss: 0.3631 - val_accuracy: 0.8844 - val_recall: 0.8844\n",
      "Epoch 4/20\n",
      "12835/12835 [==============================] - 2s 133us/sample - loss: 0.3780 - accuracy: 0.8889 - recall: 0.8881 - val_loss: 0.3133 - val_accuracy: 0.8811 - val_recall: 0.8811\n",
      "Epoch 5/20\n",
      "12835/12835 [==============================] - 2s 144us/sample - loss: 0.3289 - accuracy: 0.8918 - recall: 0.8917 - val_loss: 0.2795 - val_accuracy: 0.8951 - val_recall: 0.8951\n",
      "Epoch 6/20\n",
      "12835/12835 [==============================] - 2s 139us/sample - loss: 0.3163 - accuracy: 0.8945 - recall: 0.8951 - val_loss: 0.2903 - val_accuracy: 0.8899 - val_recall: 0.8898\n",
      "Epoch 7/20\n",
      "12835/12835 [==============================] - 2s 159us/sample - loss: 0.3090 - accuracy: 0.8959 - recall: 0.8958 - val_loss: 0.2940 - val_accuracy: 0.8931 - val_recall: 0.8931\n",
      "Epoch 8/20\n",
      "12835/12835 [==============================] - 2s 128us/sample - loss: 0.2988 - accuracy: 0.8986 - recall: 0.8991 - val_loss: 0.2695 - val_accuracy: 0.8968 - val_recall: 0.8968\n",
      "Epoch 9/20\n",
      "12835/12835 [==============================] - 1s 104us/sample - loss: 0.2978 - accuracy: 0.8988 - recall: 0.8987 - val_loss: 0.3070 - val_accuracy: 0.8964 - val_recall: 0.8964\n",
      "Epoch 10/20\n",
      "12835/12835 [==============================] - 1s 103us/sample - loss: 0.2880 - accuracy: 0.8994 - recall: 0.8995 - val_loss: 0.2651 - val_accuracy: 0.8991 - val_recall: 0.8991\n",
      "Epoch 11/20\n",
      "12835/12835 [==============================] - 1s 103us/sample - loss: 0.2793 - accuracy: 0.9011 - recall: 0.9011 - val_loss: 0.2853 - val_accuracy: 0.8893 - val_recall: 0.8893\n",
      "Epoch 12/20\n",
      "12835/12835 [==============================] - 1s 109us/sample - loss: 0.2860 - accuracy: 0.9021 - recall: 0.9022 - val_loss: 0.2565 - val_accuracy: 0.8979 - val_recall: 0.8979\n",
      "Epoch 13/20\n",
      "12835/12835 [==============================] - 1s 105us/sample - loss: 0.2803 - accuracy: 0.9035 - recall: 0.9040 - val_loss: 0.2655 - val_accuracy: 0.8991 - val_recall: 0.8991\n",
      "Epoch 14/20\n",
      "12835/12835 [==============================] - 2s 118us/sample - loss: 0.2701 - accuracy: 0.9047 - recall: 0.9046 - val_loss: 0.2652 - val_accuracy: 0.9011 - val_recall: 0.9011\n",
      "Epoch 15/20\n",
      "12835/12835 [==============================] - 1s 106us/sample - loss: 0.2617 - accuracy: 0.9055 - recall: 0.9054 - val_loss: 0.2986 - val_accuracy: 0.8784 - val_recall: 0.8784\n",
      "Epoch 16/20\n",
      "12835/12835 [==============================] - 1s 108us/sample - loss: 0.2630 - accuracy: 0.9051 - recall: 0.9054 - val_loss: 0.2713 - val_accuracy: 0.9017 - val_recall: 0.9017\n",
      "Epoch 17/20\n",
      "12835/12835 [==============================] - 1s 117us/sample - loss: 0.2662 - accuracy: 0.9049 - recall: 0.9049 - val_loss: 0.2705 - val_accuracy: 0.9017 - val_recall: 0.9017\n",
      "Epoch 18/20\n",
      "12835/12835 [==============================] - 1s 105us/sample - loss: 0.2539 - accuracy: 0.9038 - recall: 0.9036 - val_loss: 0.2636 - val_accuracy: 0.8984 - val_recall: 0.8984\n",
      "Epoch 19/20\n",
      "12835/12835 [==============================] - 1s 103us/sample - loss: 0.2559 - accuracy: 0.9053 - recall: 0.9056 - val_loss: 0.2633 - val_accuracy: 0.8979 - val_recall: 0.8979\n",
      "Epoch 20/20\n",
      "12835/12835 [==============================] - 1s 109us/sample - loss: 0.2532 - accuracy: 0.9073 - recall: 0.9071 - val_loss: 0.2507 - val_accuracy: 0.9026 - val_recall: 0.9026\n",
      "Validation Loss: 0.25065844751669686\n",
      "Validation accuracy: 0.90258086\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_validation, y_validation))\n",
    "score = model.evaluate(x_validation, y_validation, verbose=0)\n",
    "print('Validation Loss:', score[0])\n",
    "print('Validation accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning model acheived a 90% recall on train and validation, no overfitting.\n",
    "    - If we are looking at an interpretable model, we can use the decision tree model or random forest with some more hyper parameter tuning.\n",
    "    - If we are looking for a model with high performance but no intepretability, the MLP model is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_std = sc.transform(X_train)\n",
    "x_validation_std = sc.transform(X_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               20992     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 284,674\n",
      "Trainable params: 284,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['binary_crossentropy'],\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy', recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12835 samples, validate on 5502 samples\n",
      "Epoch 1/20\n",
      "12835/12835 [==============================] - 3s 213us/sample - loss: 0.2963 - accuracy: 0.8707 - recall: 0.8708 - val_loss: 0.2745 - val_accuracy: 0.8768 - val_recall: 0.8768\n",
      "Epoch 2/20\n",
      "12835/12835 [==============================] - 1s 109us/sample - loss: 0.2579 - accuracy: 0.8891 - recall: 0.8892 - val_loss: 0.2780 - val_accuracy: 0.8686 - val_recall: 0.8686\n",
      "Epoch 3/20\n",
      "12835/12835 [==============================] - 1s 108us/sample - loss: 0.2479 - accuracy: 0.8908 - recall: 0.8903 - val_loss: 0.2861 - val_accuracy: 0.8579 - val_recall: 0.8579\n",
      "Epoch 4/20\n",
      "12835/12835 [==============================] - 1s 103us/sample - loss: 0.2426 - accuracy: 0.8912 - recall: 0.8905 - val_loss: 0.2583 - val_accuracy: 0.8810 - val_recall: 0.8810\n",
      "Epoch 5/20\n",
      "12835/12835 [==============================] - 1s 101us/sample - loss: 0.2391 - accuracy: 0.8954 - recall: 0.8952 - val_loss: 0.2443 - val_accuracy: 0.8922 - val_recall: 0.8922\n",
      "Epoch 6/20\n",
      "12835/12835 [==============================] - 1s 115us/sample - loss: 0.2340 - accuracy: 0.8947 - recall: 0.8947 - val_loss: 0.2442 - val_accuracy: 0.8920 - val_recall: 0.8920\n",
      "Epoch 7/20\n",
      "12835/12835 [==============================] - 2s 125us/sample - loss: 0.2321 - accuracy: 0.8975 - recall: 0.8976 - val_loss: 0.2407 - val_accuracy: 0.8937 - val_recall: 0.8937\n",
      "Epoch 8/20\n",
      "12835/12835 [==============================] - 2s 135us/sample - loss: 0.2299 - accuracy: 0.8971 - recall: 0.8972 - val_loss: 0.2598 - val_accuracy: 0.8779 - val_recall: 0.8779\n",
      "Epoch 9/20\n",
      "12835/12835 [==============================] - 1s 104us/sample - loss: 0.2277 - accuracy: 0.9002 - recall: 0.9001 - val_loss: 0.2403 - val_accuracy: 0.8895 - val_recall: 0.8895\n",
      "Epoch 10/20\n",
      "12835/12835 [==============================] - 1s 103us/sample - loss: 0.2263 - accuracy: 0.9000 - recall: 0.9003 - val_loss: 0.2558 - val_accuracy: 0.8819 - val_recall: 0.8819\n",
      "Epoch 11/20\n",
      "12835/12835 [==============================] - 2s 126us/sample - loss: 0.2241 - accuracy: 0.8997 - recall: 0.8996 - val_loss: 0.2407 - val_accuracy: 0.8895 - val_recall: 0.8895\n",
      "Epoch 12/20\n",
      "12835/12835 [==============================] - 2s 128us/sample - loss: 0.2211 - accuracy: 0.9060 - recall: 0.9060 - val_loss: 0.2347 - val_accuracy: 0.8995 - val_recall: 0.8995\n",
      "Epoch 13/20\n",
      "12835/12835 [==============================] - 1s 106us/sample - loss: 0.2204 - accuracy: 0.9021 - recall: 0.9024 - val_loss: 0.2422 - val_accuracy: 0.8944 - val_recall: 0.8944\n",
      "Epoch 14/20\n",
      "12835/12835 [==============================] - 2s 117us/sample - loss: 0.2195 - accuracy: 0.9047 - recall: 0.9052 - val_loss: 0.2447 - val_accuracy: 0.8866 - val_recall: 0.8866\n",
      "Epoch 15/20\n",
      "12835/12835 [==============================] - 1s 104us/sample - loss: 0.2177 - accuracy: 0.9044 - recall: 0.9045 - val_loss: 0.2390 - val_accuracy: 0.9033 - val_recall: 0.9033\n",
      "Epoch 16/20\n",
      "12835/12835 [==============================] - 1s 116us/sample - loss: 0.2180 - accuracy: 0.9053 - recall: 0.9057 - val_loss: 0.2350 - val_accuracy: 0.8980 - val_recall: 0.8980\n",
      "Epoch 17/20\n",
      "12835/12835 [==============================] - 1s 113us/sample - loss: 0.2154 - accuracy: 0.9042 - recall: 0.9044 - val_loss: 0.2309 - val_accuracy: 0.9000 - val_recall: 0.9000\n",
      "Epoch 18/20\n",
      "12835/12835 [==============================] - 1s 105us/sample - loss: 0.2151 - accuracy: 0.9056 - recall: 0.9062 - val_loss: 0.2645 - val_accuracy: 0.8853 - val_recall: 0.8853\n",
      "Epoch 19/20\n",
      "12835/12835 [==============================] - 1s 104us/sample - loss: 0.2134 - accuracy: 0.9066 - recall: 0.9062 - val_loss: 0.2364 - val_accuracy: 0.8899 - val_recall: 0.8899\n",
      "Epoch 20/20\n",
      "12835/12835 [==============================] - 1s 105us/sample - loss: 0.2112 - accuracy: 0.9071 - recall: 0.9067 - val_loss: 0.2449 - val_accuracy: 0.8951 - val_recall: 0.8951\n",
      "Validation Loss: 0.2449341523499976\n",
      "Validation accuracy: 0.895129\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_std, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_validation_std, y_validation))\n",
    "score = model.evaluate(x_validation_std, y_validation, verbose=0)\n",
    "print('Validation Loss:', score[0])\n",
    "print('Validation accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance with stardardized data is similar to that of the model built without standardization.\n",
    "Recall of ~90% on both train and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with tweaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_recall\", patience=30, min_delta=0.5)\n",
    "filepath=\"model_weights/weights-improvement-{epoch:02d}-recall{val_recall:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_recall', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csvl = CSVLogger(\n",
    "    filename='training_logs/training.log',\n",
    "    separator=',', \n",
    "    append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [es,checkpoint,csvl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12835, 40)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 512)               20992     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 284,674\n",
      "Trainable params: 284,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12835 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "12736/12835 [============================>.] - ETA: 0s - loss: 17.6941 - accuracy: 0.8452 - recall: 0.8449\n",
      "Epoch 00001: val_recall did not improve from 0.90549\n",
      "12835/12835 [==============================] - 3s 197us/sample - loss: 17.5660 - accuracy: 0.8455 - recall: 0.8452 - val_loss: 9.3523 - val_accuracy: 0.8669 - val_recall: 0.8773\n",
      "Epoch 2/1000\n",
      "12544/12835 [============================>.] - ETA: 0s - loss: 17.1161 - accuracy: 0.8509 - recall: 0.8522\n",
      "Epoch 00002: val_recall did not improve from 0.90549\n",
      "12835/12835 [==============================] - 2s 141us/sample - loss: 16.7958 - accuracy: 0.8512 - recall: 0.8524 - val_loss: 29.3576 - val_accuracy: 0.8848 - val_recall: 0.8788\n",
      "Epoch 3/1000\n",
      "12800/12835 [============================>.] - ETA: 0s - loss: 5.0840 - accuracy: 0.8446 - recall: 0.8397\n",
      "Epoch 00003: val_recall did not improve from 0.90549\n",
      "12835/12835 [==============================] - 2s 130us/sample - loss: 5.0715 - accuracy: 0.8449 - recall: 0.8402 - val_loss: 27.5942 - val_accuracy: 0.8601 - val_recall: 0.8688\n",
      "Epoch 4/1000\n",
      "12672/12835 [============================>.] - ETA: 0s - loss: 324.5959 - accuracy: 0.8618 - recall: 0.8636\n",
      "Epoch 00004: val_recall did not improve from 0.90549\n",
      "12835/12835 [==============================] - 2s 130us/sample - loss: 320.5619 - accuracy: 0.8622 - recall: 0.8639 - val_loss: 104.7544 - val_accuracy: 0.8876 - val_recall: 0.8953\n",
      "Epoch 5/1000\n",
      "12736/12835 [============================>.] - ETA: 0s - loss: 2679.4245 - accuracy: 0.8496 - recall: 0.8558\n",
      "Epoch 00005: val_recall improved from 0.90549 to 0.90822, saving model to model_weights/weights-improvement-05-recall0.91.hdf5\n",
      "12835/12835 [==============================] - 2s 145us/sample - loss: 2659.4298 - accuracy: 0.8496 - recall: 0.8559 - val_loss: 159.0117 - val_accuracy: 0.8744 - val_recall: 0.9082\n",
      "Epoch 6/1000\n",
      "12608/12835 [============================>.] - ETA: 0s - loss: 12.0781 - accuracy: 0.8543 - recall: 0.8552\n",
      "Epoch 00006: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 140us/sample - loss: 11.9570 - accuracy: 0.8533 - recall: 0.8530 - val_loss: 74.8369 - val_accuracy: 0.8715 - val_recall: 0.8323\n",
      "Epoch 7/1000\n",
      "12800/12835 [============================>.] - ETA: 0s - loss: 4.2951 - accuracy: 0.8685 - recall: 0.8655\n",
      "Epoch 00007: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 138us/sample - loss: 4.2841 - accuracy: 0.8684 - recall: 0.8655 - val_loss: 55.8138 - val_accuracy: 0.8823 - val_recall: 0.8809\n",
      "Epoch 8/1000\n",
      "12672/12835 [============================>.] - ETA: 0s - loss: 3.5951 - accuracy: 0.8792 - recall: 0.8725\n",
      "Epoch 00008: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 130us/sample - loss: 3.5557 - accuracy: 0.8798 - recall: 0.8735 - val_loss: 34.9867 - val_accuracy: 0.8853 - val_recall: 0.8844\n",
      "Epoch 9/1000\n",
      "12736/12835 [============================>.] - ETA: 0s - loss: 3.3582 - accuracy: 0.8883 - recall: 0.8889\n",
      "Epoch 00009: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 129us/sample - loss: 3.3353 - accuracy: 0.8881 - recall: 0.8887 - val_loss: 34.2230 - val_accuracy: 0.8797 - val_recall: 0.8826\n",
      "Epoch 10/1000\n",
      "12736/12835 [============================>.] - ETA: 0s - loss: 1.6763 - accuracy: 0.8851 - recall: 0.8842\n",
      "Epoch 00010: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 130us/sample - loss: 1.6647 - accuracy: 0.8854 - recall: 0.8847 - val_loss: 27.7184 - val_accuracy: 0.8843 - val_recall: 0.8846\n",
      "Epoch 11/1000\n",
      "12736/12835 [============================>.] - ETA: 0s - loss: 0.9658 - accuracy: 0.8910 - recall: 0.8902\n",
      "Epoch 00011: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 129us/sample - loss: 0.9609 - accuracy: 0.8911 - recall: 0.8904 - val_loss: 23.7426 - val_accuracy: 0.8890 - val_recall: 0.8877\n",
      "Epoch 12/1000\n",
      "12352/12835 [===========================>..] - ETA: 0s - loss: 5.5452 - accuracy: 0.8891 - recall: 0.8902\n",
      "Epoch 00012: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 143us/sample - loss: 5.4021 - accuracy: 0.8893 - recall: 0.8901 - val_loss: 50.0219 - val_accuracy: 0.8895 - val_recall: 0.8871\n",
      "Epoch 13/1000\n",
      "12544/12835 [============================>.] - ETA: 0s - loss: 1.4040 - accuracy: 0.8915 - recall: 0.8905\n",
      "Epoch 00013: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 133us/sample - loss: 1.3783 - accuracy: 0.8917 - recall: 0.8907 - val_loss: 36.2323 - val_accuracy: 0.8887 - val_recall: 0.8877\n",
      "Epoch 14/1000\n",
      "12480/12835 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.8938 - recall: 0.8938\n",
      "Epoch 00014: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 132us/sample - loss: 0.5535 - accuracy: 0.8939 - recall: 0.8939 - val_loss: 27.9724 - val_accuracy: 0.8887 - val_recall: 0.8873\n",
      "Epoch 15/1000\n",
      "12416/12835 [============================>.] - ETA: 0s - loss: 0.5594 - accuracy: 0.8919 - recall: 0.8904\n",
      "Epoch 00015: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 131us/sample - loss: 0.5484 - accuracy: 0.8927 - recall: 0.8914 - val_loss: 25.9796 - val_accuracy: 0.8934 - val_recall: 0.8919\n",
      "Epoch 16/1000\n",
      "12480/12835 [============================>.] - ETA: 0s - loss: 0.6778 - accuracy: 0.8933 - recall: 0.8925\n",
      "Epoch 00016: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 138us/sample - loss: 0.6671 - accuracy: 0.8932 - recall: 0.8924 - val_loss: 23.0916 - val_accuracy: 0.8971 - val_recall: 0.8969\n",
      "Epoch 17/1000\n",
      "12672/12835 [============================>.] - ETA: 0s - loss: 0.4257 - accuracy: 0.8943 - recall: 0.8950\n",
      "Epoch 00017: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 143us/sample - loss: 0.4237 - accuracy: 0.8940 - recall: 0.8948 - val_loss: 19.1591 - val_accuracy: 0.8949 - val_recall: 0.8960\n",
      "Epoch 18/1000\n",
      "12672/12835 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.8954 - recall: 0.8976\n",
      "Epoch 00018: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 149us/sample - loss: 0.2687 - accuracy: 0.8953 - recall: 0.8972 - val_loss: 18.3681 - val_accuracy: 0.8966 - val_recall: 0.8955\n",
      "Epoch 19/1000\n",
      "12672/12835 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.8957 - recall: 0.8966\n",
      "Epoch 00019: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 131us/sample - loss: 0.2896 - accuracy: 0.8959 - recall: 0.8967 - val_loss: 17.5722 - val_accuracy: 0.8834 - val_recall: 0.8810\n",
      "Epoch 20/1000\n",
      "12352/12835 [===========================>..] - ETA: 0s - loss: 0.2949 - accuracy: 0.8949 - recall: 0.8943\n",
      "Epoch 00020: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 128us/sample - loss: 0.2977 - accuracy: 0.8951 - recall: 0.8946 - val_loss: 15.6448 - val_accuracy: 0.8913 - val_recall: 0.8924\n",
      "Epoch 21/1000\n",
      "12800/12835 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8978 - recall: 0.8979\n",
      "Epoch 00021: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 130us/sample - loss: 0.3664 - accuracy: 0.8975 - recall: 0.8974 - val_loss: 21.4474 - val_accuracy: 0.8971 - val_recall: 0.8979\n",
      "Epoch 22/1000\n",
      "12352/12835 [===========================>..] - ETA: 0s - loss: 0.4057 - accuracy: 0.9003 - recall: 0.9034\n",
      "Epoch 00022: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 129us/sample - loss: 0.4025 - accuracy: 0.8984 - recall: 0.9011 - val_loss: 15.5129 - val_accuracy: 0.8923 - val_recall: 0.8859\n",
      "Epoch 23/1000\n",
      "12352/12835 [===========================>..] - ETA: 0s - loss: 0.4012 - accuracy: 0.8986 - recall: 0.8981\n",
      "Epoch 00023: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 130us/sample - loss: 0.3968 - accuracy: 0.8982 - recall: 0.8976 - val_loss: 11.7510 - val_accuracy: 0.8915 - val_recall: 0.8911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000\n",
      "12800/12835 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.9000 - recall: 0.8995\n",
      "Epoch 00024: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 129us/sample - loss: 0.3091 - accuracy: 0.8999 - recall: 0.8995 - val_loss: 8.7722 - val_accuracy: 0.8971 - val_recall: 0.8971\n",
      "Epoch 25/1000\n",
      "12544/12835 [============================>.] - ETA: 0s - loss: 0.2674 - accuracy: 0.8984 - recall: 0.8996\n",
      "Epoch 00025: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 132us/sample - loss: 0.2679 - accuracy: 0.8982 - recall: 0.8994 - val_loss: 8.6760 - val_accuracy: 0.8984 - val_recall: 0.8973\n",
      "Epoch 26/1000\n",
      "12672/12835 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.8982 - recall: 0.8984\n",
      "Epoch 00026: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 131us/sample - loss: 0.2741 - accuracy: 0.8983 - recall: 0.8986 - val_loss: 7.9959 - val_accuracy: 0.8969 - val_recall: 0.8970\n",
      "Epoch 27/1000\n",
      "12544/12835 [============================>.] - ETA: 0s - loss: 0.2673 - accuracy: 0.8986 - recall: 0.8986\n",
      "Epoch 00027: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 132us/sample - loss: 0.2656 - accuracy: 0.8993 - recall: 0.8992 - val_loss: 7.2974 - val_accuracy: 0.8973 - val_recall: 0.8955\n",
      "Epoch 28/1000\n",
      "12352/12835 [===========================>..] - ETA: 0s - loss: 0.2525 - accuracy: 0.9005 - recall: 0.8996\n",
      "Epoch 00028: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 130us/sample - loss: 0.2514 - accuracy: 0.9005 - recall: 0.8998 - val_loss: 7.3414 - val_accuracy: 0.8989 - val_recall: 0.8984\n",
      "Epoch 29/1000\n",
      "12416/12835 [============================>.] - ETA: 0s - loss: 0.2552 - accuracy: 0.9000 - recall: 0.8992\n",
      "Epoch 00029: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 134us/sample - loss: 0.2543 - accuracy: 0.8999 - recall: 0.8989 - val_loss: 7.3175 - val_accuracy: 0.8989 - val_recall: 0.8980\n",
      "Epoch 30/1000\n",
      "12416/12835 [============================>.] - ETA: 0s - loss: 0.2514 - accuracy: 0.8997 - recall: 0.8991\n",
      "Epoch 00030: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 136us/sample - loss: 0.2510 - accuracy: 0.8998 - recall: 0.8991 - val_loss: 7.4363 - val_accuracy: 0.8997 - val_recall: 0.8991\n",
      "Epoch 31/1000\n",
      "12608/12835 [============================>.] - ETA: 0s - loss: 0.2530 - accuracy: 0.9001 - recall: 0.9004\n",
      "Epoch 00031: val_recall did not improve from 0.90822\n",
      "12835/12835 [==============================] - 2s 133us/sample - loss: 0.2527 - accuracy: 0.9002 - recall: 0.9005 - val_loss: 8.2889 - val_accuracy: 0.8992 - val_recall: 0.8991\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=['binary_crossentropy'],\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy', recall])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_validation, y_validation),callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.28888676851522\n",
      "Validation accuracy: 0.89921844\n",
      "Validation Recall: 0.89914\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_validation, y_validation, verbose=0)\n",
    "print('Validation Loss:', score[0])\n",
    "print('Validation accuracy:', score[1])\n",
    "print('Validation Recall:', score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.28888676851522, 0.89921844, 0.89914]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation and train recall are ~90% I have only done a dropout and early stopping, building a deeper netwrok may improve the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training metrics changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.read_csv(\"training_logs/training.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.845462</td>\n",
       "      <td>17.566042</td>\n",
       "      <td>0.845229</td>\n",
       "      <td>0.866867</td>\n",
       "      <td>9.352285</td>\n",
       "      <td>0.877321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.851227</td>\n",
       "      <td>16.795758</td>\n",
       "      <td>0.852419</td>\n",
       "      <td>0.884769</td>\n",
       "      <td>29.357563</td>\n",
       "      <td>0.878763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.844877</td>\n",
       "      <td>5.071482</td>\n",
       "      <td>0.840201</td>\n",
       "      <td>0.860051</td>\n",
       "      <td>27.594191</td>\n",
       "      <td>0.868782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.862213</td>\n",
       "      <td>320.561889</td>\n",
       "      <td>0.863886</td>\n",
       "      <td>0.887586</td>\n",
       "      <td>104.754415</td>\n",
       "      <td>0.895308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.849552</td>\n",
       "      <td>2659.429828</td>\n",
       "      <td>0.855930</td>\n",
       "      <td>0.874409</td>\n",
       "      <td>159.011708</td>\n",
       "      <td>0.908219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  accuracy         loss    recall  val_accuracy    val_loss  \\\n",
       "0      0  0.845462    17.566042  0.845229      0.866867    9.352285   \n",
       "1      1  0.851227    16.795758  0.852419      0.884769   29.357563   \n",
       "2      2  0.844877     5.071482  0.840201      0.860051   27.594191   \n",
       "3      3  0.862213   320.561889  0.863886      0.887586  104.754415   \n",
       "4      4  0.849552  2659.429828  0.855930      0.874409  159.011708   \n",
       "\n",
       "   val_recall  \n",
       "0    0.877321  \n",
       "1    0.878763  \n",
       "2    0.868782  \n",
       "3    0.895308  \n",
       "4    0.908219  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.offline as pyoff\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "y": [
          0.8454616,
          0.8512271,
          0.8448773,
          0.8622127,
          0.84955204,
          0.85333073,
          0.8684066999999999,
          0.8798208000000001,
          0.88807946,
          0.88539153,
          0.89111805,
          0.88932604,
          0.89170235,
          0.8938839,
          0.8926763000000001,
          0.8932216999999999,
          0.89403975,
          0.8952863000000001,
          0.8959096,
          0.8951305,
          0.89754575,
          0.8984028000000001,
          0.898208,
          0.8999221,
          0.8982469999999999,
          0.8983249,
          0.89925987,
          0.90054536,
          0.8999221,
          0.89976627,
          0.90023375
         ]
        },
        {
         "name": "loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "y": [
          17.566041602273835,
          16.795758146049913,
          5.071482292148544,
          320.56188903774415,
          2659.4298280448324,
          11.956963888383072,
          4.284120565757337,
          3.555708860227186,
          3.3353306378749816,
          1.6647455987377209,
          0.9608878945626309,
          5.402065690473028,
          1.3782954113071049,
          0.553513054365679,
          0.5483561657075989,
          0.6670977946473877,
          0.4237181559390812,
          0.26870901521932394,
          0.2896022462466369,
          0.29768554890346194,
          0.36636798562453704,
          0.4025107491048896,
          0.3967535605725956,
          0.3090955317879541,
          0.2679422433106036,
          0.2741413083959285,
          0.2656117941195927,
          0.25144371022196377,
          0.2543052644454695,
          0.2509595911582713,
          0.2527468095226979
         ]
        },
        {
         "name": "recall",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "y": [
          0.8452291999999999,
          0.8524187,
          0.8402008,
          0.8638859,
          0.85593015,
          0.85304284,
          0.8655050999999999,
          0.87353635,
          0.8886705,
          0.8846527,
          0.8903673000000001,
          0.8901475000000001,
          0.8906783000000001,
          0.8938521999999999,
          0.8914156999999999,
          0.89238846,
          0.8948494,
          0.8972215,
          0.89672846,
          0.8945651,
          0.8974036,
          0.9011083000000001,
          0.8976101999999999,
          0.8994625,
          0.8994492000000001,
          0.89858073,
          0.899216,
          0.8997601000000001,
          0.89886725,
          0.8991383,
          0.90047306
         ]
        },
        {
         "name": "val_accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "y": [
          0.8668665999999999,
          0.8847691999999999,
          0.8600509,
          0.88758636,
          0.8744093000000001,
          0.87150127,
          0.8823155,
          0.8853144000000001,
          0.8796801,
          0.8843148000000001,
          0.88904035,
          0.8894947,
          0.8886768,
          0.8886768,
          0.8934023999999999,
          0.89712834,
          0.8948564,
          0.8965831,
          0.88340604,
          0.89131224,
          0.89712834,
          0.8923119,
          0.891494,
          0.89712834,
          0.8984006,
          0.8968556999999999,
          0.8973101000000001,
          0.8989458,
          0.8989458,
          0.89967287,
          0.89921844
         ]
        },
        {
         "name": "val_loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "y": [
          9.352285124440057,
          29.357563320007035,
          27.594191438282504,
          104.75441494164836,
          159.01170840626497,
          74.8368733156295,
          55.8138013350253,
          34.98673252106493,
          34.223039246545795,
          27.7183532730552,
          23.742562423377247,
          50.02189086888409,
          36.232339009588436,
          27.97241879426557,
          25.979599967804752,
          23.091551081559434,
          19.159135996103718,
          18.368087777844213,
          17.572241441856207,
          15.64480762774621,
          21.44739221484932,
          15.512919738699159,
          11.750989046493515,
          8.77217637364841,
          8.676008675135035,
          7.995941191272535,
          7.297400481676631,
          7.341387842236455,
          7.317518792312519,
          7.436345440719137,
          8.288888675576771
         ]
        },
        {
         "name": "val_recall",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "y": [
          0.87732095,
          0.8787626000000001,
          0.8687816999999999,
          0.89530784,
          0.9082193000000001,
          0.83225685,
          0.8809486999999999,
          0.8844066999999999,
          0.88258386,
          0.88458836,
          0.8876652,
          0.8871319999999999,
          0.887677,
          0.88731366,
          0.89185584,
          0.8969488999999999,
          0.89604044,
          0.8954953999999999,
          0.8809546999999999,
          0.8924066999999999,
          0.8978573000000001,
          0.88586605,
          0.8911349000000001,
          0.89713055,
          0.8973064000000001,
          0.8969546999999999,
          0.8954895,
          0.8984023999999999,
          0.8980448000000001,
          0.899135,
          0.899135
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training loss and accuracy changes"
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"53f8ac56-8533-46db-b780-a5de27268eb2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"53f8ac56-8533-46db-b780-a5de27268eb2\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '53f8ac56-8533-46db-b780-a5de27268eb2',\n",
       "                        [{\"name\": \"accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"y\": [0.8454616, 0.8512271, 0.8448773, 0.8622127, 0.84955204, 0.85333073, 0.8684066999999999, 0.8798208000000001, 0.88807946, 0.88539153, 0.89111805, 0.88932604, 0.89170235, 0.8938839, 0.8926763000000001, 0.8932216999999999, 0.89403975, 0.8952863000000001, 0.8959096, 0.8951305, 0.89754575, 0.8984028000000001, 0.898208, 0.8999221, 0.8982469999999999, 0.8983249, 0.89925987, 0.90054536, 0.8999221, 0.89976627, 0.90023375]}, {\"name\": \"loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"y\": [17.566041602273835, 16.795758146049913, 5.071482292148544, 320.56188903774415, 2659.4298280448324, 11.956963888383072, 4.284120565757337, 3.555708860227186, 3.3353306378749816, 1.6647455987377209, 0.9608878945626309, 5.402065690473028, 1.3782954113071049, 0.553513054365679, 0.5483561657075989, 0.6670977946473877, 0.4237181559390812, 0.26870901521932394, 0.2896022462466369, 0.29768554890346194, 0.36636798562453704, 0.4025107491048896, 0.3967535605725956, 0.3090955317879541, 0.2679422433106036, 0.2741413083959285, 0.2656117941195927, 0.25144371022196377, 0.2543052644454695, 0.2509595911582713, 0.2527468095226979]}, {\"name\": \"recall\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"y\": [0.8452291999999999, 0.8524187, 0.8402008, 0.8638859, 0.85593015, 0.85304284, 0.8655050999999999, 0.87353635, 0.8886705, 0.8846527, 0.8903673000000001, 0.8901475000000001, 0.8906783000000001, 0.8938521999999999, 0.8914156999999999, 0.89238846, 0.8948494, 0.8972215, 0.89672846, 0.8945651, 0.8974036, 0.9011083000000001, 0.8976101999999999, 0.8994625, 0.8994492000000001, 0.89858073, 0.899216, 0.8997601000000001, 0.89886725, 0.8991383, 0.90047306]}, {\"name\": \"val_accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"y\": [0.8668665999999999, 0.8847691999999999, 0.8600509, 0.88758636, 0.8744093000000001, 0.87150127, 0.8823155, 0.8853144000000001, 0.8796801, 0.8843148000000001, 0.88904035, 0.8894947, 0.8886768, 0.8886768, 0.8934023999999999, 0.89712834, 0.8948564, 0.8965831, 0.88340604, 0.89131224, 0.89712834, 0.8923119, 0.891494, 0.89712834, 0.8984006, 0.8968556999999999, 0.8973101000000001, 0.8989458, 0.8989458, 0.89967287, 0.89921844]}, {\"name\": \"val_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"y\": [9.352285124440057, 29.357563320007035, 27.594191438282504, 104.75441494164836, 159.01170840626497, 74.8368733156295, 55.8138013350253, 34.98673252106493, 34.223039246545795, 27.7183532730552, 23.742562423377247, 50.02189086888409, 36.232339009588436, 27.97241879426557, 25.979599967804752, 23.091551081559434, 19.159135996103718, 18.368087777844213, 17.572241441856207, 15.64480762774621, 21.44739221484932, 15.512919738699159, 11.750989046493515, 8.77217637364841, 8.676008675135035, 7.995941191272535, 7.297400481676631, 7.341387842236455, 7.317518792312519, 7.436345440719137, 8.288888675576771]}, {\"name\": \"val_recall\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"y\": [0.87732095, 0.8787626000000001, 0.8687816999999999, 0.89530784, 0.9082193000000001, 0.83225685, 0.8809486999999999, 0.8844066999999999, 0.88258386, 0.88458836, 0.8876652, 0.8871319999999999, 0.887677, 0.88731366, 0.89185584, 0.8969488999999999, 0.89604044, 0.8954953999999999, 0.8809546999999999, 0.8924066999999999, 0.8978573000000001, 0.88586605, 0.8911349000000001, 0.89713055, 0.8973064000000001, 0.8969546999999999, 0.8954895, 0.8984023999999999, 0.8980448000000001, 0.899135, 0.899135]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training loss and accuracy changes\"}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('53f8ac56-8533-46db-b780-a5de27268eb2');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data  = []\n",
    "for i in logs.columns:\n",
    "    if i != \"epoch\":\n",
    "        data.append(go.Scatter(x=logs.epoch, y=logs[i], name = i))\n",
    "\n",
    "layout = go.Layout(title =\"Training loss and accuracy changes\")        \n",
    "fig = go.Figure(data = data, layout= layout)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep MLP with random grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_recall\", patience=10, min_delta=0.5)\n",
    "filepath=\"model_weights/weights-improvement-gs-{epoch:02d}-recall{val_recall:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_recall', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csvl = CSVLogger(\n",
    "    filename='training_logs/grid_search.log',\n",
    "    separator=',', \n",
    "    append=False)\n",
    "\n",
    "callbacks = [es,checkpoint,csvl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(optimizer='adam', dense_dims1=512,dense_dims2 = 128, dense_dims3 = 64, \n",
    "               dropout1 = 0.2, dropout2 = 0.2, dropout3 = 0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_dims1, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dropout(dropout1))\n",
    "    model.add(Dense(dense_dims2, activation='relu'))\n",
    "    model.add(Dropout(dropout2))\n",
    "    model.add(Dense(dense_dims3, activation='relu'))\n",
    "    model.add(Dropout(dropout3))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', recall])\n",
    "    return model\n",
    "\n",
    "keras_classifier = KerasClassifier(build_model, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the grid space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = [1000]\n",
    "dense_dims1 = [128,512]\n",
    "dense_dims2 = [128,512]\n",
    "dense_dims3 = [128,512]\n",
    "dropout1 = np.linspace(0,0.4,2)\n",
    "dropout2 = np.linspace(0,0.4,2)\n",
    "dropout3 = np.linspace(0,0.4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = RandomizedSearchCV(keras_classifier, {'epochs': n_epochs, 'dense_dims1':dense_dims1,\n",
    "                                     'dense_dims2':dense_dims2,\n",
    "                                     'dense_dims3':dense_dims3,\n",
    "                                     \n",
    "                                    'dropout1' :dropout1,\n",
    "                                    'dropout2' :dropout2,\n",
    "                                    'dropout3' :dropout3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': [1000],\n",
       " 'dense_dims1': [128, 512],\n",
       " 'dense_dims2': [128, 512],\n",
       " 'dense_dims3': [128, 512],\n",
       " 'dropout1': array([0. , 0.4]),\n",
       " 'dropout2': array([0. , 0.4]),\n",
       " 'dropout3': array([0. , 0.4])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.param_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.8717 - accuracy: 0.8690 - recall: 0.8659\n",
      "Epoch 00001: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 295us/sample - loss: 0.8693 - accuracy: 0.8694 - recall: 0.8659 - val_loss: 0.6733 - val_accuracy: 0.8808 - val_recall: 0.8352\n",
      "Epoch 2/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.8865 - recall: 0.8871\n",
      "Epoch 00002: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.4223 - accuracy: 0.8867 - recall: 0.8876 - val_loss: 0.2985 - val_accuracy: 0.8831 - val_recall: 0.8880\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8965 - recall: 0.8968\n",
      "Epoch 00003: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.3102 - accuracy: 0.8974 - recall: 0.8977 - val_loss: 0.2677 - val_accuracy: 0.8939 - val_recall: 0.8949\n",
      "Epoch 4/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.9004 - recall: 0.9005\n",
      "Epoch 00004: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 275us/sample - loss: 0.2601 - accuracy: 0.9007 - recall: 0.9008 - val_loss: 0.2580 - val_accuracy: 0.8949 - val_recall: 0.9035\n",
      "Epoch 5/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9012 - recall: 0.9010\n",
      "Epoch 00005: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 263us/sample - loss: 0.2571 - accuracy: 0.9006 - recall: 0.9004 - val_loss: 0.2618 - val_accuracy: 0.8969 - val_recall: 0.8989\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.9028 - recall: 0.9037\n",
      "Epoch 00006: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 260us/sample - loss: 0.2466 - accuracy: 0.9028 - recall: 0.9036 - val_loss: 0.2678 - val_accuracy: 0.8884 - val_recall: 0.8879\n",
      "Epoch 7/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9053 - recall: 0.9048\n",
      "Epoch 00007: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.2364 - accuracy: 0.9059 - recall: 0.9054 - val_loss: 0.2499 - val_accuracy: 0.8969 - val_recall: 0.8984\n",
      "Epoch 8/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9084 - recall: 0.9088\n",
      "Epoch 00008: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 271us/sample - loss: 0.2331 - accuracy: 0.9070 - recall: 0.9074 - val_loss: 0.2666 - val_accuracy: 0.8991 - val_recall: 0.9006\n",
      "Epoch 9/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2429 - accuracy: 0.9059 - recall: 0.9058\n",
      "Epoch 00009: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.2425 - accuracy: 0.9061 - recall: 0.9061 - val_loss: 0.2466 - val_accuracy: 0.8998 - val_recall: 0.8996\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2298 - accuracy: 0.9069 - recall: 0.9075\n",
      "Epoch 00010: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2298 - accuracy: 0.9069 - recall: 0.9076 - val_loss: 0.2396 - val_accuracy: 0.9001 - val_recall: 0.9000\n",
      "Epoch 11/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9065 - recall: 0.9074\n",
      "Epoch 00011: val_recall did not improve from 0.92422\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2302 - accuracy: 0.9063 - recall: 0.9072 - val_loss: 0.2488 - val_accuracy: 0.8988 - val_recall: 0.8980\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 108us/sample - loss: 0.2496 - accuracy: 0.9038 - recall: 0.9029\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.9060 - accuracy: 0.8563 - recall: 0.8558\n",
      "Epoch 00001: val_recall improved from 0.92422 to 0.93840, saving model to model_weights/weights-improvement-gs-01-recall0.94.hdf5\n",
      "10268/10268 [==============================] - 3s 277us/sample - loss: 0.8994 - accuracy: 0.8563 - recall: 0.8556 - val_loss: 0.7032 - val_accuracy: 0.8625 - val_recall: 0.9384\n",
      "Epoch 2/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.4381 - accuracy: 0.8807 - recall: 0.8830\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.4372 - accuracy: 0.8808 - recall: 0.8831 - val_loss: 0.3315 - val_accuracy: 0.8919 - val_recall: 0.8971\n",
      "Epoch 3/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8946 - recall: 0.8937\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 272us/sample - loss: 0.2940 - accuracy: 0.8947 - recall: 0.8937 - val_loss: 0.2616 - val_accuracy: 0.8919 - val_recall: 0.8895\n",
      "Epoch 4/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.8981 - recall: 0.8983\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.3203 - accuracy: 0.8979 - recall: 0.8981 - val_loss: 0.2821 - val_accuracy: 0.8914 - val_recall: 0.8883\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.8987 - recall: 0.9014\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.2900 - accuracy: 0.8993 - recall: 0.9020 - val_loss: 0.2584 - val_accuracy: 0.8952 - val_recall: 0.8968\n",
      "Epoch 6/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.9042 - recall: 0.9049\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2538 - accuracy: 0.9044 - recall: 0.9049 - val_loss: 0.2662 - val_accuracy: 0.8898 - val_recall: 0.8901\n",
      "Epoch 7/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.9026 - recall: 0.9021\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.2569 - accuracy: 0.9021 - recall: 0.9015 - val_loss: 0.3110 - val_accuracy: 0.8929 - val_recall: 0.8840\n",
      "Epoch 8/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.9042 - recall: 0.9035\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 278us/sample - loss: 0.2547 - accuracy: 0.9047 - recall: 0.9039 - val_loss: 0.2516 - val_accuracy: 0.8968 - val_recall: 0.8980\n",
      "Epoch 9/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9068 - recall: 0.9078\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 280us/sample - loss: 0.2332 - accuracy: 0.9068 - recall: 0.9078 - val_loss: 0.2425 - val_accuracy: 0.8951 - val_recall: 0.8947\n",
      "Epoch 10/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2457 - accuracy: 0.9075 - recall: 0.9077\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 270us/sample - loss: 0.2446 - accuracy: 0.9078 - recall: 0.9080 - val_loss: 0.2505 - val_accuracy: 0.8983 - val_recall: 0.8995\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2309 - accuracy: 0.9070 - recall: 0.9069\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 269us/sample - loss: 0.2305 - accuracy: 0.9073 - recall: 0.9072 - val_loss: 0.2418 - val_accuracy: 0.8989 - val_recall: 0.8986\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 108us/sample - loss: 0.2313 - accuracy: 0.9001 - recall: 0.9012\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.8180 - accuracy: 0.8695 - recall: 0.8717\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 297us/sample - loss: 0.8041 - accuracy: 0.8703 - recall: 0.8730 - val_loss: 0.3899 - val_accuracy: 0.8764 - val_recall: 0.8702\n",
      "Epoch 2/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.4250 - accuracy: 0.8900 - recall: 0.8920\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 337us/sample - loss: 0.4247 - accuracy: 0.8900 - recall: 0.8921 - val_loss: 0.3135 - val_accuracy: 0.8872 - val_recall: 0.8981\n",
      "Epoch 3/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.8988 - recall: 0.8978\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 302us/sample - loss: 0.2748 - accuracy: 0.8986 - recall: 0.8975 - val_loss: 0.2755 - val_accuracy: 0.8931 - val_recall: 0.8969\n",
      "Epoch 4/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.8981 - recall: 0.8990\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 271us/sample - loss: 0.2763 - accuracy: 0.8985 - recall: 0.8993 - val_loss: 0.3280 - val_accuracy: 0.8887 - val_recall: 0.8740\n",
      "Epoch 5/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9020 - recall: 0.9024\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 300us/sample - loss: 0.2865 - accuracy: 0.9018 - recall: 0.9022 - val_loss: 0.2675 - val_accuracy: 0.8939 - val_recall: 0.8902\n",
      "Epoch 6/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9048 - recall: 0.9034\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 261us/sample - loss: 0.2421 - accuracy: 0.9049 - recall: 0.9035 - val_loss: 0.2573 - val_accuracy: 0.8952 - val_recall: 0.8868\n",
      "Epoch 7/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9053 - recall: 0.9023\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.2380 - accuracy: 0.9060 - recall: 0.9030 - val_loss: 0.2770 - val_accuracy: 0.8982 - val_recall: 0.8988\n",
      "Epoch 8/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9070 - recall: 0.9060\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 271us/sample - loss: 0.2370 - accuracy: 0.9073 - recall: 0.9062 - val_loss: 0.2628 - val_accuracy: 0.8979 - val_recall: 0.8957\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2286 - accuracy: 0.9072 - recall: 0.9070\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 263us/sample - loss: 0.2282 - accuracy: 0.9073 - recall: 0.9071 - val_loss: 0.2493 - val_accuracy: 0.8981 - val_recall: 0.8993\n",
      "Epoch 10/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.9093 - recall: 0.9098\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.2291 - accuracy: 0.9092 - recall: 0.9097 - val_loss: 0.2456 - val_accuracy: 0.9004 - val_recall: 0.8999\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.9080 - recall: 0.9075\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.2282 - accuracy: 0.9080 - recall: 0.9075 - val_loss: 0.2377 - val_accuracy: 0.9024 - val_recall: 0.9015\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 96us/sample - loss: 0.2463 - accuracy: 0.8954 - recall: 0.8951\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.7365 - accuracy: 0.8705 - recall: 0.8742\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 272us/sample - loss: 0.7286 - accuracy: 0.8713 - recall: 0.8747 - val_loss: 0.4312 - val_accuracy: 0.8850 - val_recall: 0.8888\n",
      "Epoch 2/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8900 - recall: 0.8901\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 238us/sample - loss: 0.3218 - accuracy: 0.8892 - recall: 0.8893 - val_loss: 0.2820 - val_accuracy: 0.8886 - val_recall: 0.8980\n",
      "Epoch 3/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8963 - recall: 0.8978\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 261us/sample - loss: 0.3025 - accuracy: 0.8964 - recall: 0.8979 - val_loss: 0.3832 - val_accuracy: 0.8860 - val_recall: 0.8813\n",
      "Epoch 4/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8962 - recall: 0.8973\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.2942 - accuracy: 0.8963 - recall: 0.8974 - val_loss: 0.2544 - val_accuracy: 0.8934 - val_recall: 0.8941\n",
      "Epoch 5/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2665 - accuracy: 0.9015 - recall: 0.9007- ETA: 0s - loss: 0.2702 - accuracy: 0.9004 - reca\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.2673 - accuracy: 0.9012 - recall: 0.9006 - val_loss: 0.2741 - val_accuracy: 0.8934 - val_recall: 0.8930\n",
      "Epoch 6/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9044 - recall: 0.9057\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2461 - accuracy: 0.9042 - recall: 0.9055 - val_loss: 0.2593 - val_accuracy: 0.8989 - val_recall: 0.8993\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.9023 - recall: 0.9030\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 249us/sample - loss: 0.2530 - accuracy: 0.9025 - recall: 0.9033 - val_loss: 0.2598 - val_accuracy: 0.9008 - val_recall: 0.9027\n",
      "Epoch 8/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9047 - recall: 0.9041\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.2372 - accuracy: 0.9048 - recall: 0.9042 - val_loss: 0.2501 - val_accuracy: 0.8948 - val_recall: 0.8942\n",
      "Epoch 9/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9079 - recall: 0.9078\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2295 - accuracy: 0.9076 - recall: 0.9077 - val_loss: 0.2369 - val_accuracy: 0.9022 - val_recall: 0.9039\n",
      "Epoch 10/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.2268 - accuracy: 0.9067 - recall: 0.9061\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.2284 - accuracy: 0.9062 - recall: 0.9058 - val_loss: 0.2642 - val_accuracy: 0.9029 - val_recall: 0.9033\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2287 - accuracy: 0.9085 - recall: 0.9071\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 280us/sample - loss: 0.2293 - accuracy: 0.9085 - recall: 0.9072 - val_loss: 0.2411 - val_accuracy: 0.9038 - val_recall: 0.9023\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 110us/sample - loss: 0.2490 - accuracy: 0.9100 - recall: 0.9083\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.8775 - accuracy: 0.8598 - recall: 0.8544\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 305us/sample - loss: 0.8678 - accuracy: 0.8602 - recall: 0.8553 - val_loss: 0.4515 - val_accuracy: 0.8859 - val_recall: 0.8720\n",
      "Epoch 2/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.8833 - recall: 0.8795\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 260us/sample - loss: 0.3728 - accuracy: 0.8834 - recall: 0.8797 - val_loss: 0.3062 - val_accuracy: 0.8866 - val_recall: 0.8933\n",
      "Epoch 3/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.8959 - recall: 0.8988\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.2931 - accuracy: 0.8955 - recall: 0.8982 - val_loss: 0.3091 - val_accuracy: 0.8903 - val_recall: 0.8850\n",
      "Epoch 4/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.8990 - recall: 0.8994\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 272us/sample - loss: 0.2607 - accuracy: 0.8982 - recall: 0.8987 - val_loss: 0.2777 - val_accuracy: 0.8870 - val_recall: 0.8959\n",
      "Epoch 5/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2600 - accuracy: 0.9011 - recall: 0.9013\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 265us/sample - loss: 0.2604 - accuracy: 0.9011 - recall: 0.9015 - val_loss: 0.2776 - val_accuracy: 0.8919 - val_recall: 0.8971\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.8979 - recall: 0.8979\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 278us/sample - loss: 0.2522 - accuracy: 0.8979 - recall: 0.8979 - val_loss: 0.2569 - val_accuracy: 0.8974 - val_recall: 0.8987\n",
      "Epoch 7/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.9026 - recall: 0.9032\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 269us/sample - loss: 0.2538 - accuracy: 0.9022 - recall: 0.9029 - val_loss: 0.2456 - val_accuracy: 0.9000 - val_recall: 0.9028\n",
      "Epoch 8/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2402 - accuracy: 0.9039 - recall: 0.9041\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 286us/sample - loss: 0.2398 - accuracy: 0.9040 - recall: 0.9042 - val_loss: 0.2508 - val_accuracy: 0.9011 - val_recall: 0.9004\n",
      "Epoch 9/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9040 - recall: 0.9030\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 284us/sample - loss: 0.2358 - accuracy: 0.9040 - recall: 0.9029 - val_loss: 0.2452 - val_accuracy: 0.9006 - val_recall: 0.9013\n",
      "Epoch 10/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9047 - recall: 0.9055\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.2327 - accuracy: 0.9051 - recall: 0.9058 - val_loss: 0.2628 - val_accuracy: 0.8982 - val_recall: 0.8982\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.9034 - recall: 0.9042\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 269us/sample - loss: 0.2328 - accuracy: 0.9036 - recall: 0.9044 - val_loss: 0.2416 - val_accuracy: 0.8967 - val_recall: 0.8968\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 123us/sample - loss: 0.2206 - accuracy: 0.9110 - recall: 0.9117\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.7850 - accuracy: 0.8609 - recall: 0.8559\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 320us/sample - loss: 0.7795 - accuracy: 0.8607 - recall: 0.8555 - val_loss: 0.3645 - val_accuracy: 0.8798 - val_recall: 0.8769\n",
      "Epoch 2/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3872 - accuracy: 0.8848 - recall: 0.8838\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 272us/sample - loss: 0.3876 - accuracy: 0.8850 - recall: 0.8839 - val_loss: 0.2842 - val_accuracy: 0.8880 - val_recall: 0.8860\n",
      "Epoch 3/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8953 - recall: 0.8954\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 300us/sample - loss: 0.3001 - accuracy: 0.8954 - recall: 0.8956 - val_loss: 0.2746 - val_accuracy: 0.8876 - val_recall: 0.8924\n",
      "Epoch 4/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.8993 - recall: 0.8973\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 278us/sample - loss: 0.2690 - accuracy: 0.8993 - recall: 0.8973 - val_loss: 0.2437 - val_accuracy: 0.8973 - val_recall: 0.8986\n",
      "Epoch 5/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2486 - accuracy: 0.9011 - recall: 0.9006\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 260us/sample - loss: 0.2493 - accuracy: 0.9011 - recall: 0.9006 - val_loss: 0.2657 - val_accuracy: 0.8912 - val_recall: 0.8895\n",
      "Epoch 6/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9052 - recall: 0.9051\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2434 - accuracy: 0.9052 - recall: 0.9051 - val_loss: 0.2450 - val_accuracy: 0.8984 - val_recall: 0.8970\n",
      "Epoch 7/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2369 - accuracy: 0.9053 - recall: 0.9051\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.2371 - accuracy: 0.9050 - recall: 0.9049 - val_loss: 0.2517 - val_accuracy: 0.8976 - val_recall: 0.8968\n",
      "Epoch 8/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2338 - accuracy: 0.9067 - recall: 0.9072\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 264us/sample - loss: 0.2346 - accuracy: 0.9061 - recall: 0.9065 - val_loss: 0.2411 - val_accuracy: 0.8994 - val_recall: 0.8989\n",
      "Epoch 9/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2312 - accuracy: 0.9064 - recall: 0.9082\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 269us/sample - loss: 0.2320 - accuracy: 0.9060 - recall: 0.9078 - val_loss: 0.2361 - val_accuracy: 0.9004 - val_recall: 0.9022\n",
      "Epoch 10/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9067 - recall: 0.9085\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 276us/sample - loss: 0.2273 - accuracy: 0.9066 - recall: 0.9084 - val_loss: 0.2396 - val_accuracy: 0.8990 - val_recall: 0.9008\n",
      "Epoch 11/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9085 - recall: 0.9092\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 4s 369us/sample - loss: 0.2276 - accuracy: 0.9089 - recall: 0.9095 - val_loss: 0.2387 - val_accuracy: 0.9005 - val_recall: 0.9015\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 155us/sample - loss: 0.2343 - accuracy: 0.9049 - recall: 0.9064\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.8372 - accuracy: 0.8621 - recall: 0.8627\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 281us/sample - loss: 0.8280 - accuracy: 0.8621 - recall: 0.8629 - val_loss: 0.4209 - val_accuracy: 0.8828 - val_recall: 0.8977\n",
      "Epoch 2/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8844 - recall: 0.8839- ETA: 1s - loss: 0.4640 - \n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 260us/sample - loss: 0.3636 - accuracy: 0.8848 - recall: 0.8844 - val_loss: 0.2697 - val_accuracy: 0.8916 - val_recall: 0.8924\n",
      "Epoch 3/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8962 - recall: 0.8971- ETA: 1s - loss: 0.3294 - \n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2945 - accuracy: 0.8964 - recall: 0.8972 - val_loss: 0.2635 - val_accuracy: 0.8907 - val_recall: 0.8945\n",
      "Epoch 4/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.9001 - recall: 0.8988\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2674 - accuracy: 0.9002 - recall: 0.8989 - val_loss: 0.2619 - val_accuracy: 0.8945 - val_recall: 0.8944\n",
      "Epoch 5/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8989 - recall: 0.8995\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.2587 - accuracy: 0.8990 - recall: 0.8995 - val_loss: 0.2599 - val_accuracy: 0.8977 - val_recall: 0.8993\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9011 - recall: 0.9006\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2507 - accuracy: 0.9009 - recall: 0.9004 - val_loss: 0.2432 - val_accuracy: 0.8975 - val_recall: 0.8973\n",
      "Epoch 7/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2481 - accuracy: 0.9029 - recall: 0.9025\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.2487 - accuracy: 0.9027 - recall: 0.9023 - val_loss: 0.2494 - val_accuracy: 0.8986 - val_recall: 0.8992\n",
      "Epoch 8/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9046 - recall: 0.9036\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.2440 - accuracy: 0.9045 - recall: 0.9035 - val_loss: 0.2464 - val_accuracy: 0.8976 - val_recall: 0.8969\n",
      "Epoch 9/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2338 - accuracy: 0.9059 - recall: 0.9048\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2336 - accuracy: 0.9060 - recall: 0.9048 - val_loss: 0.2514 - val_accuracy: 0.8986 - val_recall: 0.8993\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9072 - recall: 0.9071\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 262us/sample - loss: 0.2356 - accuracy: 0.9073 - recall: 0.9073 - val_loss: 0.2364 - val_accuracy: 0.9009 - val_recall: 0.8997\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9086 - recall: 0.9076\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.2280 - accuracy: 0.9086 - recall: 0.9076 - val_loss: 0.2435 - val_accuracy: 0.8996 - val_recall: 0.8984\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 100us/sample - loss: 0.2279 - accuracy: 0.9014 - recall: 0.9001\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.8319 - accuracy: 0.8673 - recall: 0.8679\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 298us/sample - loss: 0.8307 - accuracy: 0.8674 - recall: 0.8682 - val_loss: 0.4538 - val_accuracy: 0.8818 - val_recall: 0.8806\n",
      "Epoch 2/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8819 - recall: 0.8833\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 266us/sample - loss: 0.4056 - accuracy: 0.8827 - recall: 0.8842 - val_loss: 0.2849 - val_accuracy: 0.8898 - val_recall: 0.8961\n",
      "Epoch 3/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.8947 - recall: 0.8951\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 300us/sample - loss: 0.3171 - accuracy: 0.8950 - recall: 0.8954 - val_loss: 0.2808 - val_accuracy: 0.8919 - val_recall: 0.8924\n",
      "Epoch 4/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2775 - accuracy: 0.8980 - recall: 0.9041\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 293us/sample - loss: 0.2779 - accuracy: 0.8974 - recall: 0.9034 - val_loss: 0.2553 - val_accuracy: 0.8921 - val_recall: 0.8944\n",
      "Epoch 5/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.9023 - recall: 0.9055\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 272us/sample - loss: 0.2537 - accuracy: 0.9027 - recall: 0.9059 - val_loss: 0.2478 - val_accuracy: 0.8898 - val_recall: 0.8921\n",
      "Epoch 6/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2432 - accuracy: 0.9032 - recall: 0.9060\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 262us/sample - loss: 0.2421 - accuracy: 0.9033 - recall: 0.9061 - val_loss: 0.2488 - val_accuracy: 0.8968 - val_recall: 0.8973\n",
      "Epoch 7/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.9067 - recall: 0.9079\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.2366 - accuracy: 0.9056 - recall: 0.9067 - val_loss: 0.2452 - val_accuracy: 0.8969 - val_recall: 0.8977\n",
      "Epoch 8/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2331 - accuracy: 0.9043 - recall: 0.9051\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2334 - accuracy: 0.9048 - recall: 0.9055 - val_loss: 0.2538 - val_accuracy: 0.8969 - val_recall: 0.8978\n",
      "Epoch 9/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2299 - accuracy: 0.9084 - recall: 0.9097\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2291 - accuracy: 0.9086 - recall: 0.9099 - val_loss: 0.2564 - val_accuracy: 0.8943 - val_recall: 0.8952\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2320 - accuracy: 0.9067 - recall: 0.9069\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 243us/sample - loss: 0.2307 - accuracy: 0.9071 - recall: 0.9074 - val_loss: 0.2577 - val_accuracy: 0.8951 - val_recall: 0.8951\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2378 - accuracy: 0.9042 - recall: 0.9044\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2379 - accuracy: 0.9041 - recall: 0.9042 - val_loss: 0.2483 - val_accuracy: 0.8981 - val_recall: 0.8986\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 101us/sample - loss: 0.2525 - accuracy: 0.8923 - recall: 0.8925\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.8087 - accuracy: 0.8633 - recall: 0.8653\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 279us/sample - loss: 0.8073 - accuracy: 0.8634 - recall: 0.8654 - val_loss: 0.4678 - val_accuracy: 0.8632 - val_recall: 0.8722\n",
      "Epoch 2/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.4038 - accuracy: 0.8811 - recall: 0.8808\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.4010 - accuracy: 0.8822 - recall: 0.8820 - val_loss: 0.3117 - val_accuracy: 0.8844 - val_recall: 0.8864\n",
      "Epoch 3/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.8911 - recall: 0.8917\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.3076 - accuracy: 0.8908 - recall: 0.8915 - val_loss: 0.2692 - val_accuracy: 0.8899 - val_recall: 0.8881\n",
      "Epoch 4/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2677 - accuracy: 0.8981 - recall: 0.8972\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 249us/sample - loss: 0.2684 - accuracy: 0.8983 - recall: 0.8972 - val_loss: 0.2581 - val_accuracy: 0.8929 - val_recall: 0.8908\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2665 - accuracy: 0.8963 - recall: 0.8947\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 252us/sample - loss: 0.2657 - accuracy: 0.8969 - recall: 0.8954 - val_loss: 0.2621 - val_accuracy: 0.8968 - val_recall: 0.8964\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.9029 - recall: 0.9034\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2547 - accuracy: 0.9032 - recall: 0.9038 - val_loss: 0.2524 - val_accuracy: 0.8895 - val_recall: 0.8897\n",
      "Epoch 7/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9013 - recall: 0.9031\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2438 - accuracy: 0.9005 - recall: 0.9023 - val_loss: 0.2521 - val_accuracy: 0.8954 - val_recall: 0.8977\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9036 - recall: 0.9046\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2386 - accuracy: 0.9032 - recall: 0.9041 - val_loss: 0.2565 - val_accuracy: 0.8919 - val_recall: 0.8931\n",
      "Epoch 9/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.9065 - recall: 0.9070\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2308 - accuracy: 0.9063 - recall: 0.9068 - val_loss: 0.2486 - val_accuracy: 0.8998 - val_recall: 0.9029\n",
      "Epoch 10/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.9040 - recall: 0.9045\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2352 - accuracy: 0.9048 - recall: 0.9053 - val_loss: 0.2368 - val_accuracy: 0.8993 - val_recall: 0.8997\n",
      "Epoch 11/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2304 - accuracy: 0.9057 - recall: 0.9063\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2305 - accuracy: 0.9056 - recall: 0.9063 - val_loss: 0.2535 - val_accuracy: 0.9005 - val_recall: 0.9002\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 102us/sample - loss: 0.2479 - accuracy: 0.9083 - recall: 0.9080\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.8545 - accuracy: 0.8573 - recall: 0.8581\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 278us/sample - loss: 0.8471 - accuracy: 0.8579 - recall: 0.8584 - val_loss: 0.4295 - val_accuracy: 0.8841 - val_recall: 0.8637\n",
      "Epoch 2/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.4147 - accuracy: 0.8802 - recall: 0.8770\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.4138 - accuracy: 0.8804 - recall: 0.8772 - val_loss: 0.3747 - val_accuracy: 0.8909 - val_recall: 0.8868\n",
      "Epoch 3/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8907 - recall: 0.8903\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.3390 - accuracy: 0.8905 - recall: 0.8900 - val_loss: 0.2751 - val_accuracy: 0.8865 - val_recall: 0.8789\n",
      "Epoch 4/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2809 - accuracy: 0.8939 - recall: 0.8930\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.2799 - accuracy: 0.8942 - recall: 0.8933 - val_loss: 0.2521 - val_accuracy: 0.8972 - val_recall: 0.8977\n",
      "Epoch 5/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2623 - accuracy: 0.8971 - recall: 0.8971\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2619 - accuracy: 0.8972 - recall: 0.8972 - val_loss: 0.2507 - val_accuracy: 0.8960 - val_recall: 0.8986\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.8978 - recall: 0.8951\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.2520 - accuracy: 0.8979 - recall: 0.8952 - val_loss: 0.2502 - val_accuracy: 0.8987 - val_recall: 0.8962\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9021 - recall: 0.9008\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2489 - accuracy: 0.9019 - recall: 0.9005 - val_loss: 0.2390 - val_accuracy: 0.8991 - val_recall: 0.9006\n",
      "Epoch 8/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.9040 - recall: 0.9031\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2430 - accuracy: 0.9036 - recall: 0.9027 - val_loss: 0.2413 - val_accuracy: 0.9007 - val_recall: 0.8993\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.9044 - recall: 0.9030\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2356 - accuracy: 0.9045 - recall: 0.9031 - val_loss: 0.2401 - val_accuracy: 0.8939 - val_recall: 0.8933\n",
      "Epoch 10/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.9038 - recall: 0.9022\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 249us/sample - loss: 0.2338 - accuracy: 0.9043 - recall: 0.9027 - val_loss: 0.2362 - val_accuracy: 0.9009 - val_recall: 0.8996\n",
      "Epoch 11/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9066 - recall: 0.9056\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 249us/sample - loss: 0.2308 - accuracy: 0.9063 - recall: 0.9051 - val_loss: 0.2360 - val_accuracy: 0.9007 - val_recall: 0.9013\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 103us/sample - loss: 0.2145 - accuracy: 0.9131 - recall: 0.9147\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.8684 - recall: 0.8664\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 243us/sample - loss: 0.5604 - accuracy: 0.8681 - recall: 0.8662 - val_loss: 0.3586 - val_accuracy: 0.8683 - val_recall: 0.8686\n",
      "Epoch 2/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8858 - recall: 0.8910\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.3253 - accuracy: 0.8856 - recall: 0.8907 - val_loss: 0.2742 - val_accuracy: 0.8876 - val_recall: 0.8932\n",
      "Epoch 3/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8892 - recall: 0.8912\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2918 - accuracy: 0.8898 - recall: 0.8918 - val_loss: 0.2740 - val_accuracy: 0.8845 - val_recall: 0.8880\n",
      "Epoch 4/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8925 - recall: 0.8938\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.2873 - accuracy: 0.8924 - recall: 0.8937 - val_loss: 0.2751 - val_accuracy: 0.8936 - val_recall: 0.8951\n",
      "Epoch 5/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2673 - accuracy: 0.8984 - recall: 0.8981\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.2678 - accuracy: 0.8982 - recall: 0.8979 - val_loss: 0.2553 - val_accuracy: 0.8960 - val_recall: 0.8959\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2673 - accuracy: 0.8989 - recall: 0.8997\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.2686 - accuracy: 0.8988 - recall: 0.8995 - val_loss: 0.2631 - val_accuracy: 0.8783 - val_recall: 0.8778\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.8961 - recall: 0.8961\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2606 - accuracy: 0.8960 - recall: 0.8960 - val_loss: 0.2469 - val_accuracy: 0.8936 - val_recall: 0.8939\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2566 - accuracy: 0.8982 - recall: 0.8987\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2576 - accuracy: 0.8984 - recall: 0.8988 - val_loss: 0.2758 - val_accuracy: 0.8910 - val_recall: 0.8922\n",
      "Epoch 9/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2495 - accuracy: 0.9024 - recall: 0.9026\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2492 - accuracy: 0.9023 - recall: 0.9025 - val_loss: 0.2480 - val_accuracy: 0.8979 - val_recall: 0.8982\n",
      "Epoch 10/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9014 - recall: 0.9027\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.2438 - accuracy: 0.9014 - recall: 0.9027 - val_loss: 0.2445 - val_accuracy: 0.8906 - val_recall: 0.8906\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2362 - accuracy: 0.9035 - recall: 0.9039\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2362 - accuracy: 0.9036 - recall: 0.9040 - val_loss: 0.2466 - val_accuracy: 0.9000 - val_recall: 0.9000\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 96us/sample - loss: 0.2444 - accuracy: 0.9028 - recall: 0.9022\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.6578 - accuracy: 0.8666 - recall: 0.8681\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 263us/sample - loss: 0.6555 - accuracy: 0.8664 - recall: 0.8679 - val_loss: 0.2944 - val_accuracy: 0.8849 - val_recall: 0.8868\n",
      "Epoch 2/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.8858 - recall: 0.8876\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 221us/sample - loss: 0.3270 - accuracy: 0.8861 - recall: 0.8879 - val_loss: 0.2713 - val_accuracy: 0.8885 - val_recall: 0.8886\n",
      "Epoch 3/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8892 - recall: 0.8899\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 225us/sample - loss: 0.2957 - accuracy: 0.8892 - recall: 0.8901 - val_loss: 0.2727 - val_accuracy: 0.8921 - val_recall: 0.8945\n",
      "Epoch 4/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.8948 - recall: 0.8956\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 220us/sample - loss: 0.2767 - accuracy: 0.8948 - recall: 0.8955 - val_loss: 0.2536 - val_accuracy: 0.8948 - val_recall: 0.8975\n",
      "Epoch 5/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.8956 - recall: 0.8956\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2722 - accuracy: 0.8955 - recall: 0.8955 - val_loss: 0.2543 - val_accuracy: 0.8946 - val_recall: 0.8949\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.8980 - recall: 0.8980\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 221us/sample - loss: 0.2660 - accuracy: 0.8981 - recall: 0.8980 - val_loss: 0.2730 - val_accuracy: 0.8907 - val_recall: 0.8900\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2624 - accuracy: 0.8985 - recall: 0.8988\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2626 - accuracy: 0.8985 - recall: 0.8988 - val_loss: 0.2536 - val_accuracy: 0.8962 - val_recall: 0.8969\n",
      "Epoch 8/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2631 - accuracy: 0.9000 - recall: 0.9001\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2631 - accuracy: 0.9000 - recall: 0.9002 - val_loss: 0.2692 - val_accuracy: 0.8930 - val_recall: 0.8930\n",
      "Epoch 9/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2550 - accuracy: 0.9010 - recall: 0.9015\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2546 - accuracy: 0.9014 - recall: 0.9019 - val_loss: 0.2583 - val_accuracy: 0.8993 - val_recall: 0.8993\n",
      "Epoch 10/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2457 - accuracy: 0.9031 - recall: 0.9033\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2453 - accuracy: 0.9031 - recall: 0.9034 - val_loss: 0.2424 - val_accuracy: 0.8981 - val_recall: 0.8985\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.9039 - recall: 0.9041\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 231us/sample - loss: 0.2503 - accuracy: 0.9038 - recall: 0.9040 - val_loss: 0.2470 - val_accuracy: 0.8986 - val_recall: 0.8995\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 99us/sample - loss: 0.2366 - accuracy: 0.9012 - recall: 0.9024\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.6096 - accuracy: 0.8696 - recall: 0.8719\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 258us/sample - loss: 0.6079 - accuracy: 0.8696 - recall: 0.8719 - val_loss: 0.2879 - val_accuracy: 0.8793 - val_recall: 0.8804\n",
      "Epoch 2/1000\n",
      " 9920/10268 [===========================>..] - ETA: 0s - loss: 0.3496 - accuracy: 0.8879 - recall: 0.8854\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 212us/sample - loss: 0.3493 - accuracy: 0.8872 - recall: 0.8846 - val_loss: 0.3555 - val_accuracy: 0.8869 - val_recall: 0.8839\n",
      "Epoch 3/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8910 - recall: 0.8890- ETA: 0s - loss: 0.3071 - accuracy: 0.8919 - \n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.3038 - accuracy: 0.8907 - recall: 0.8886 - val_loss: 0.2636 - val_accuracy: 0.8902 - val_recall: 0.8919\n",
      "Epoch 4/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8929 - recall: 0.8935\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2823 - accuracy: 0.8927 - recall: 0.8932 - val_loss: 0.2678 - val_accuracy: 0.8899 - val_recall: 0.8901\n",
      "Epoch 5/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2731 - accuracy: 0.8965 - recall: 0.8959- ETA: 0s - loss: 0.2749 - accuracy: 0.8958 - recall: 0\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2729 - accuracy: 0.8969 - recall: 0.8963 - val_loss: 0.2618 - val_accuracy: 0.8901 - val_recall: 0.8899\n",
      "Epoch 6/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.8994 - recall: 0.8985\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 227us/sample - loss: 0.2828 - accuracy: 0.8994 - recall: 0.8985 - val_loss: 0.2709 - val_accuracy: 0.8947 - val_recall: 0.8950\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.8999 - recall: 0.8993\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 223us/sample - loss: 0.2617 - accuracy: 0.8999 - recall: 0.8993 - val_loss: 0.2496 - val_accuracy: 0.8928 - val_recall: 0.8917\n",
      "Epoch 8/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9037 - recall: 0.9032\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 234us/sample - loss: 0.2448 - accuracy: 0.9044 - recall: 0.9039 - val_loss: 0.2476 - val_accuracy: 0.8966 - val_recall: 0.8959\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9030 - recall: 0.9032\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 270us/sample - loss: 0.2465 - accuracy: 0.9029 - recall: 0.9031 - val_loss: 0.2632 - val_accuracy: 0.8989 - val_recall: 0.8991\n",
      "Epoch 10/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9066 - recall: 0.9067\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 296us/sample - loss: 0.2402 - accuracy: 0.9061 - recall: 0.9063 - val_loss: 0.2418 - val_accuracy: 0.8956 - val_recall: 0.8957\n",
      "Epoch 11/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9036 - recall: 0.9034\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.2360 - accuracy: 0.9038 - recall: 0.9036 - val_loss: 0.2405 - val_accuracy: 0.8941 - val_recall: 0.8938\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 99us/sample - loss: 0.2509 - accuracy: 0.8921 - recall: 0.8914\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.5508 - accuracy: 0.8713 - recall: 0.8688\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.5469 - accuracy: 0.8719 - recall: 0.8695 - val_loss: 0.3535 - val_accuracy: 0.8732 - val_recall: 0.8750\n",
      "Epoch 2/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3241 - accuracy: 0.8890 - recall: 0.8888\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 264us/sample - loss: 0.3231 - accuracy: 0.8898 - recall: 0.8896 - val_loss: 0.3122 - val_accuracy: 0.8810 - val_recall: 0.8811\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8942 - recall: 0.8941\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 229us/sample - loss: 0.2943 - accuracy: 0.8941 - recall: 0.8939 - val_loss: 0.2628 - val_accuracy: 0.8876 - val_recall: 0.8873\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2668 - accuracy: 0.8969 - recall: 0.8974\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 238us/sample - loss: 0.2668 - accuracy: 0.8968 - recall: 0.8973 - val_loss: 0.2572 - val_accuracy: 0.8927 - val_recall: 0.8926\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8977 - recall: 0.8981\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 4s 342us/sample - loss: 0.2639 - accuracy: 0.8980 - recall: 0.8983 - val_loss: 0.2547 - val_accuracy: 0.8882 - val_recall: 0.8904\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.8966 - recall: 0.8967\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 277us/sample - loss: 0.2592 - accuracy: 0.8970 - recall: 0.8971 - val_loss: 0.2694 - val_accuracy: 0.8947 - val_recall: 0.8951\n",
      "Epoch 7/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9012 - recall: 0.9015\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 293us/sample - loss: 0.2491 - accuracy: 0.9014 - recall: 0.9018 - val_loss: 0.2462 - val_accuracy: 0.8989 - val_recall: 0.8998\n",
      "Epoch 8/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.8998 - recall: 0.9001\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2514 - accuracy: 0.8995 - recall: 0.8998 - val_loss: 0.2447 - val_accuracy: 0.8927 - val_recall: 0.8920\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.9027 - recall: 0.9021\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2495 - accuracy: 0.9025 - recall: 0.9018 - val_loss: 0.2649 - val_accuracy: 0.8987 - val_recall: 0.8984\n",
      "Epoch 10/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.9037 - recall: 0.9036\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2416 - accuracy: 0.9034 - recall: 0.9032 - val_loss: 0.2664 - val_accuracy: 0.8903 - val_recall: 0.8897\n",
      "Epoch 11/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.9042 - recall: 0.9034\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 280us/sample - loss: 0.2594 - accuracy: 0.9044 - recall: 0.9037 - val_loss: 0.2747 - val_accuracy: 0.8898 - val_recall: 0.8895\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 121us/sample - loss: 0.2970 - accuracy: 0.8933 - recall: 0.8939\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.6676 - accuracy: 0.8544 - recall: 0.8511\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 298us/sample - loss: 0.6652 - accuracy: 0.8537 - recall: 0.8505 - val_loss: 0.3333 - val_accuracy: 0.8550 - val_recall: 0.8551\n",
      "Epoch 2/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8602 - recall: 0.8601\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.3395 - accuracy: 0.8602 - recall: 0.8600 - val_loss: 0.3107 - val_accuracy: 0.8550 - val_recall: 0.8549\n",
      "Epoch 3/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8691 - recall: 0.8804\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 233us/sample - loss: 0.2986 - accuracy: 0.8697 - recall: 0.8809 - val_loss: 0.2735 - val_accuracy: 0.8762 - val_recall: 0.8847\n",
      "Epoch 4/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.8842 - recall: 0.8865\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 290us/sample - loss: 0.2841 - accuracy: 0.8848 - recall: 0.8872 - val_loss: 0.2720 - val_accuracy: 0.8873 - val_recall: 0.8886\n",
      "Epoch 5/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.8917 - recall: 0.8918\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 294us/sample - loss: 0.2847 - accuracy: 0.8919 - recall: 0.8921 - val_loss: 0.2793 - val_accuracy: 0.8881 - val_recall: 0.8884\n",
      "Epoch 6/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2697 - accuracy: 0.8924 - recall: 0.8919\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 299us/sample - loss: 0.2691 - accuracy: 0.8927 - recall: 0.8922 - val_loss: 0.2587 - val_accuracy: 0.8881 - val_recall: 0.8876\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.8967 - recall: 0.8961\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 310us/sample - loss: 0.2640 - accuracy: 0.8966 - recall: 0.8960 - val_loss: 0.2631 - val_accuracy: 0.8917 - val_recall: 0.8918\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.8957 - recall: 0.8952\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2588 - accuracy: 0.8957 - recall: 0.8953 - val_loss: 0.2438 - val_accuracy: 0.8970 - val_recall: 0.8966\n",
      "Epoch 9/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.8980 - recall: 0.8973\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 252us/sample - loss: 0.2530 - accuracy: 0.8979 - recall: 0.8972 - val_loss: 0.2680 - val_accuracy: 0.8988 - val_recall: 0.8975\n",
      "Epoch 10/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.8958 - recall: 0.8951\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 262us/sample - loss: 0.2483 - accuracy: 0.8963 - recall: 0.8956 - val_loss: 0.2483 - val_accuracy: 0.8949 - val_recall: 0.8929\n",
      "Epoch 11/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2472 - accuracy: 0.9006 - recall: 0.9001\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 238us/sample - loss: 0.2482 - accuracy: 0.9001 - recall: 0.8996 - val_loss: 0.2521 - val_accuracy: 0.8938 - val_recall: 0.8933\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 119us/sample - loss: 0.2252 - accuracy: 0.9122 - recall: 0.9124\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.9425 - accuracy: 0.8528 - recall: 0.8541\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 276us/sample - loss: 0.9372 - accuracy: 0.8531 - recall: 0.8542 - val_loss: 0.6158 - val_accuracy: 0.8591 - val_recall: 0.8575\n",
      "Epoch 2/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.5517 - accuracy: 0.8598 - recall: 0.8576\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.5513 - accuracy: 0.8596 - recall: 0.8573 - val_loss: 0.3820 - val_accuracy: 0.8741 - val_recall: 0.8710\n",
      "Epoch 3/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.4512 - accuracy: 0.8683 - recall: 0.8697\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 260us/sample - loss: 0.4475 - accuracy: 0.8685 - recall: 0.8701 - val_loss: 0.2972 - val_accuracy: 0.8705 - val_recall: 0.8668\n",
      "Epoch 4/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8764 - recall: 0.8772- ETA: 1s - loss: 0.3643 - accuracy: 0.8\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 288us/sample - loss: 0.3604 - accuracy: 0.8764 - recall: 0.8772 - val_loss: 0.3006 - val_accuracy: 0.8846 - val_recall: 0.8853\n",
      "Epoch 5/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8821 - recall: 0.8816\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.3241 - accuracy: 0.8822 - recall: 0.8817 - val_loss: 0.3128 - val_accuracy: 0.8875 - val_recall: 0.8820\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.8822 - recall: 0.8828\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 284us/sample - loss: 0.3139 - accuracy: 0.8822 - recall: 0.8826 - val_loss: 0.3037 - val_accuracy: 0.8894 - val_recall: 0.8857\n",
      "Epoch 7/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.8906 - recall: 0.8902\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 272us/sample - loss: 0.3144 - accuracy: 0.8906 - recall: 0.8902 - val_loss: 0.2768 - val_accuracy: 0.8916 - val_recall: 0.8919\n",
      "Epoch 8/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8897 - recall: 0.8898\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 265us/sample - loss: 0.2968 - accuracy: 0.8897 - recall: 0.8899 - val_loss: 0.2681 - val_accuracy: 0.8908 - val_recall: 0.8918\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.8917 - recall: 0.8920\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 279us/sample - loss: 0.2865 - accuracy: 0.8916 - recall: 0.8917 - val_loss: 0.2691 - val_accuracy: 0.8834 - val_recall: 0.8848\n",
      "Epoch 10/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.8918 - recall: 0.8920\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 263us/sample - loss: 0.2863 - accuracy: 0.8917 - recall: 0.8919 - val_loss: 0.2645 - val_accuracy: 0.8889 - val_recall: 0.8915\n",
      "Epoch 11/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2731 - accuracy: 0.8957 - recall: 0.8951\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2736 - accuracy: 0.8955 - recall: 0.8948 - val_loss: 0.2586 - val_accuracy: 0.8955 - val_recall: 0.8951\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 125us/sample - loss: 0.2630 - accuracy: 0.8985 - recall: 0.8979\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.9427 - accuracy: 0.8444 - recall: 0.8402\n",
      "Epoch 00001: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 296us/sample - loss: 0.9329 - accuracy: 0.8445 - recall: 0.8407 - val_loss: 0.4434 - val_accuracy: 0.8739 - val_recall: 0.9006\n",
      "Epoch 2/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.4901 - accuracy: 0.8611 - recall: 0.8661\n",
      "Epoch 00002: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 264us/sample - loss: 0.4898 - accuracy: 0.8613 - recall: 0.8662 - val_loss: 0.3476 - val_accuracy: 0.8753 - val_recall: 0.8708\n",
      "Epoch 3/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8706 - recall: 0.8701\n",
      "Epoch 00003: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 266us/sample - loss: 0.4133 - accuracy: 0.8705 - recall: 0.8700 - val_loss: 0.3368 - val_accuracy: 0.8816 - val_recall: 0.8841\n",
      "Epoch 4/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8777 - recall: 0.8768\n",
      "Epoch 00004: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 271us/sample - loss: 0.3511 - accuracy: 0.8778 - recall: 0.8768 - val_loss: 0.2921 - val_accuracy: 0.8841 - val_recall: 0.8849\n",
      "Epoch 5/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8871 - recall: 0.8876\n",
      "Epoch 00005: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 274us/sample - loss: 0.3257 - accuracy: 0.8864 - recall: 0.8869 - val_loss: 0.2967 - val_accuracy: 0.8848 - val_recall: 0.8844\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8827 - recall: 0.8818\n",
      "Epoch 00006: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 312us/sample - loss: 0.3227 - accuracy: 0.8827 - recall: 0.8819 - val_loss: 0.2906 - val_accuracy: 0.8900 - val_recall: 0.8913\n",
      "Epoch 7/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.8878 - recall: 0.8871\n",
      "Epoch 00007: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 4s 369us/sample - loss: 0.2991 - accuracy: 0.8878 - recall: 0.8869 - val_loss: 0.2835 - val_accuracy: 0.8901 - val_recall: 0.8891\n",
      "Epoch 8/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8876 - recall: 0.8881- ETA: 2s - lo\n",
      "Epoch 00008: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 4s 367us/sample - loss: 0.2889 - accuracy: 0.8880 - recall: 0.8884 - val_loss: 0.2768 - val_accuracy: 0.8867 - val_recall: 0.8904\n",
      "Epoch 9/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.8934 - recall: 0.8924\n",
      "Epoch 00009: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 275us/sample - loss: 0.2922 - accuracy: 0.8936 - recall: 0.8926 - val_loss: 0.2713 - val_accuracy: 0.8949 - val_recall: 0.8938\n",
      "Epoch 10/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.8939 - recall: 0.8940\n",
      "Epoch 00010: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.2840 - accuracy: 0.8941 - recall: 0.8942 - val_loss: 0.2788 - val_accuracy: 0.8923 - val_recall: 0.8929\n",
      "Epoch 11/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2806 - accuracy: 0.8946 - recall: 0.8929\n",
      "Epoch 00011: val_recall did not improve from 0.93840\n",
      "10268/10268 [==============================] - 3s 278us/sample - loss: 0.2805 - accuracy: 0.8945 - recall: 0.8928 - val_loss: 0.2624 - val_accuracy: 0.8924 - val_recall: 0.8900\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 139us/sample - loss: 0.2503 - accuracy: 0.8991 - recall: 0.8974\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.9120 - accuracy: 0.8474 - recall: 0.8469\n",
      "Epoch 00001: val_recall improved from 0.93840 to 0.94130, saving model to model_weights/weights-improvement-gs-01-recall0.94.hdf5\n",
      "10268/10268 [==============================] - 3s 286us/sample - loss: 0.9100 - accuracy: 0.8473 - recall: 0.8471 - val_loss: 0.4313 - val_accuracy: 0.8488 - val_recall: 0.9413\n",
      "Epoch 2/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.8596 - recall: 0.8635\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.4795 - accuracy: 0.8592 - recall: 0.8630 - val_loss: 0.3391 - val_accuracy: 0.8690 - val_recall: 0.8749\n",
      "Epoch 3/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.4007 - accuracy: 0.8718 - recall: 0.8736\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.4038 - accuracy: 0.8717 - recall: 0.8736 - val_loss: 0.3339 - val_accuracy: 0.8745 - val_recall: 0.8691\n",
      "Epoch 4/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3626 - accuracy: 0.8741 - recall: 0.8743\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.3629 - accuracy: 0.8744 - recall: 0.8744 - val_loss: 0.3262 - val_accuracy: 0.8796 - val_recall: 0.8808\n",
      "Epoch 5/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3425 - accuracy: 0.8791 - recall: 0.8790\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.3432 - accuracy: 0.8784 - recall: 0.8785 - val_loss: 0.3296 - val_accuracy: 0.8657 - val_recall: 0.8666\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8860 - recall: 0.8858\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.3213 - accuracy: 0.8860 - recall: 0.8859 - val_loss: 0.3104 - val_accuracy: 0.8700 - val_recall: 0.8769\n",
      "Epoch 7/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8893 - recall: 0.8883\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.3057 - accuracy: 0.8895 - recall: 0.8885 - val_loss: 0.2900 - val_accuracy: 0.8900 - val_recall: 0.8871\n",
      "Epoch 8/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8918 - recall: 0.8924\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.3009 - accuracy: 0.8919 - recall: 0.8926 - val_loss: 0.2989 - val_accuracy: 0.8876 - val_recall: 0.8855\n",
      "Epoch 9/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8924 - recall: 0.8914\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 339us/sample - loss: 0.2947 - accuracy: 0.8926 - recall: 0.8917 - val_loss: 0.2815 - val_accuracy: 0.8862 - val_recall: 0.8849\n",
      "Epoch 10/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8953 - recall: 0.8934\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 275us/sample - loss: 0.2931 - accuracy: 0.8952 - recall: 0.8934 - val_loss: 0.2805 - val_accuracy: 0.8948 - val_recall: 0.8937\n",
      "Epoch 11/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.8983 - recall: 0.8975\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.2853 - accuracy: 0.8978 - recall: 0.8970 - val_loss: 0.2717 - val_accuracy: 0.8934 - val_recall: 0.8918\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 113us/sample - loss: 0.2870 - accuracy: 0.8894 - recall: 0.8871\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.9589 - accuracy: 0.8518 - recall: 0.8567\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 4s 344us/sample - loss: 0.9548 - accuracy: 0.8519 - recall: 0.8572 - val_loss: 0.4501 - val_accuracy: 0.8656 - val_recall: 0.8956\n",
      "Epoch 2/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.5007 - accuracy: 0.8538 - recall: 0.8500\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.4992 - accuracy: 0.8542 - recall: 0.8503 - val_loss: 0.3270 - val_accuracy: 0.8577 - val_recall: 0.8573\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3871 - accuracy: 0.8749 - recall: 0.8765\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.3884 - accuracy: 0.8743 - recall: 0.8762 - val_loss: 0.3335 - val_accuracy: 0.8676 - val_recall: 0.8728\n",
      "Epoch 4/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.8789 - recall: 0.8798\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.3355 - accuracy: 0.8792 - recall: 0.8800 - val_loss: 0.3070 - val_accuracy: 0.8768 - val_recall: 0.8753\n",
      "Epoch 5/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.8882 - recall: 0.8873\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 255us/sample - loss: 0.3218 - accuracy: 0.8882 - recall: 0.8872 - val_loss: 0.3008 - val_accuracy: 0.8829 - val_recall: 0.8822\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3141 - accuracy: 0.8863 - recall: 0.8826\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 243us/sample - loss: 0.3137 - accuracy: 0.8863 - recall: 0.8826 - val_loss: 0.2744 - val_accuracy: 0.8922 - val_recall: 0.8901\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.8876 - recall: 0.8867\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 283us/sample - loss: 0.3024 - accuracy: 0.8881 - recall: 0.8873 - val_loss: 0.3056 - val_accuracy: 0.8884 - val_recall: 0.8882\n",
      "Epoch 8/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8884 - recall: 0.8887\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 273us/sample - loss: 0.3003 - accuracy: 0.8886 - recall: 0.8889 - val_loss: 0.2758 - val_accuracy: 0.8897 - val_recall: 0.8900\n",
      "Epoch 9/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8890 - recall: 0.8887\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 269us/sample - loss: 0.3012 - accuracy: 0.8891 - recall: 0.8890 - val_loss: 0.2722 - val_accuracy: 0.8803 - val_recall: 0.8815\n",
      "Epoch 10/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8925 - recall: 0.8935\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2890 - accuracy: 0.8924 - recall: 0.8933 - val_loss: 0.2821 - val_accuracy: 0.8858 - val_recall: 0.8866\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.8932 - recall: 0.8936\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.2889 - accuracy: 0.8934 - recall: 0.8939 - val_loss: 0.2799 - val_accuracy: 0.8903 - val_recall: 0.8917\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 143us/sample - loss: 0.2767 - accuracy: 0.8927 - recall: 0.8927\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.9984 - accuracy: 0.8494 - recall: 0.8536\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 319us/sample - loss: 0.9943 - accuracy: 0.8495 - recall: 0.8538 - val_loss: 0.5081 - val_accuracy: 0.8633 - val_recall: 0.8433\n",
      "Epoch 2/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.5429 - accuracy: 0.8493 - recall: 0.8329\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.5407 - accuracy: 0.8499 - recall: 0.8338 - val_loss: 0.3648 - val_accuracy: 0.8550 - val_recall: 0.8550\n",
      "Epoch 3/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.4066 - accuracy: 0.8654 - recall: 0.8642\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.4050 - accuracy: 0.8654 - recall: 0.8640 - val_loss: 0.2973 - val_accuracy: 0.8823 - val_recall: 0.8786\n",
      "Epoch 4/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.8747 - recall: 0.8752\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 260us/sample - loss: 0.3796 - accuracy: 0.8740 - recall: 0.8746 - val_loss: 0.3086 - val_accuracy: 0.8869 - val_recall: 0.8875\n",
      "Epoch 5/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3472 - accuracy: 0.8787 - recall: 0.8791\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 265us/sample - loss: 0.3475 - accuracy: 0.8786 - recall: 0.8791 - val_loss: 0.2849 - val_accuracy: 0.8906 - val_recall: 0.8904\n",
      "Epoch 6/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8812 - recall: 0.8809\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 278us/sample - loss: 0.3210 - accuracy: 0.8808 - recall: 0.8806 - val_loss: 0.3114 - val_accuracy: 0.8806 - val_recall: 0.8791\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8818 - recall: 0.8838\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.3226 - accuracy: 0.8815 - recall: 0.8835 - val_loss: 0.2786 - val_accuracy: 0.8894 - val_recall: 0.8929\n",
      "Epoch 8/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8865 - recall: 0.8882\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.3071 - accuracy: 0.8866 - recall: 0.8884 - val_loss: 0.2839 - val_accuracy: 0.8857 - val_recall: 0.8879\n",
      "Epoch 9/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3028 - accuracy: 0.8876 - recall: 0.8889\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 272us/sample - loss: 0.3021 - accuracy: 0.8882 - recall: 0.8896 - val_loss: 0.2927 - val_accuracy: 0.8680 - val_recall: 0.8724\n",
      "Epoch 10/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.8876 - recall: 0.8872\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.2999 - accuracy: 0.8881 - recall: 0.8878 - val_loss: 0.2793 - val_accuracy: 0.8869 - val_recall: 0.8908\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8890 - recall: 0.891 - ETA: 0s - loss: 0.2948 - accuracy: 0.8895 - recall: 0.8917\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 288us/sample - loss: 0.2945 - accuracy: 0.8898 - recall: 0.8918 - val_loss: 0.2703 - val_accuracy: 0.8835 - val_recall: 0.8846\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 118us/sample - loss: 0.2398 - accuracy: 0.9129 - recall: 0.9151\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.5243 - accuracy: 0.8672 - recall: 0.8654\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 302us/sample - loss: 0.5242 - accuracy: 0.8671 - recall: 0.8651 - val_loss: 0.3168 - val_accuracy: 0.8847 - val_recall: 0.8917\n",
      "Epoch 2/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.8869 - recall: 0.8865\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 271us/sample - loss: 0.3501 - accuracy: 0.8871 - recall: 0.8868 - val_loss: 0.2923 - val_accuracy: 0.8872 - val_recall: 0.8886\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8872 - recall: 0.8872\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.3051 - accuracy: 0.8882 - recall: 0.8882 - val_loss: 0.2888 - val_accuracy: 0.8909 - val_recall: 0.8908\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8919 - recall: 0.8919\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 238us/sample - loss: 0.2959 - accuracy: 0.8910 - recall: 0.8911 - val_loss: 0.2980 - val_accuracy: 0.8848 - val_recall: 0.8891\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.8933 - recall: 0.8940\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.2858 - accuracy: 0.8934 - recall: 0.8942 - val_loss: 0.2881 - val_accuracy: 0.8912 - val_recall: 0.8966\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8936 - recall: 0.8923\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.2928 - accuracy: 0.8939 - recall: 0.8927 - val_loss: 0.2911 - val_accuracy: 0.8868 - val_recall: 0.8888\n",
      "Epoch 7/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.8971 - recall: 0.8972\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 273us/sample - loss: 0.2896 - accuracy: 0.8972 - recall: 0.8974 - val_loss: 0.2818 - val_accuracy: 0.8945 - val_recall: 0.8970\n",
      "Epoch 8/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.8981 - recall: 0.8979\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.2785 - accuracy: 0.8982 - recall: 0.8979 - val_loss: 0.2742 - val_accuracy: 0.8949 - val_recall: 0.8962\n",
      "Epoch 9/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2641 - accuracy: 0.9010 - recall: 0.9011\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.2642 - accuracy: 0.9010 - recall: 0.9011 - val_loss: 0.2644 - val_accuracy: 0.8983 - val_recall: 0.8953\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2801 - accuracy: 0.8991 - recall: 0.8971\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.2835 - accuracy: 0.8987 - recall: 0.8968 - val_loss: 0.4716 - val_accuracy: 0.8935 - val_recall: 0.9026\n",
      "Epoch 11/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2649 - accuracy: 0.9020 - recall: 0.9043- ETA: 0s - loss: 0.2706 - accuracy: 0.8992 - reca\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.2637 - accuracy: 0.9025 - recall: 0.9046 - val_loss: 0.2556 - val_accuracy: 0.8984 - val_recall: 0.9002\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 110us/sample - loss: 0.2500 - accuracy: 0.9036 - recall: 0.9068\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.5340 - accuracy: 0.8676 - recall: 0.8699\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 297us/sample - loss: 0.5315 - accuracy: 0.8676 - recall: 0.8699 - val_loss: 0.3479 - val_accuracy: 0.8848 - val_recall: 0.8857\n",
      "Epoch 2/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3347 - accuracy: 0.8818 - recall: 0.8830\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 293us/sample - loss: 0.3350 - accuracy: 0.8820 - recall: 0.8831 - val_loss: 0.3351 - val_accuracy: 0.8826 - val_recall: 0.8828\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8819 - recall: 0.8834\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.3355 - accuracy: 0.8814 - recall: 0.8828 - val_loss: 0.2900 - val_accuracy: 0.8860 - val_recall: 0.8867\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8871 - recall: 0.8874\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.3125 - accuracy: 0.8873 - recall: 0.8877 - val_loss: 0.2950 - val_accuracy: 0.8854 - val_recall: 0.8919\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8884 - recall: 0.8879\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.3253 - accuracy: 0.8882 - recall: 0.8877 - val_loss: 0.2797 - val_accuracy: 0.8926 - val_recall: 0.8915\n",
      "Epoch 6/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3335 - accuracy: 0.8918 - recall: 0.8934\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 264us/sample - loss: 0.3343 - accuracy: 0.8914 - recall: 0.8930 - val_loss: 0.2914 - val_accuracy: 0.8915 - val_recall: 0.8911\n",
      "Epoch 7/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.8965 - recall: 0.8964\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.2893 - accuracy: 0.8962 - recall: 0.8962 - val_loss: 0.2771 - val_accuracy: 0.8905 - val_recall: 0.8964\n",
      "Epoch 8/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2774 - accuracy: 0.8996 - recall: 0.9002\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.2765 - accuracy: 0.8999 - recall: 0.9005 - val_loss: 0.2886 - val_accuracy: 0.8919 - val_recall: 0.8880\n",
      "Epoch 9/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.9020 - recall: 0.9025\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 291us/sample - loss: 0.2700 - accuracy: 0.9019 - recall: 0.9023 - val_loss: 0.2715 - val_accuracy: 0.8947 - val_recall: 0.8933\n",
      "Epoch 10/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2745 - accuracy: 0.9006 - recall: 0.8997\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 252us/sample - loss: 0.2734 - accuracy: 0.9008 - recall: 0.8999 - val_loss: 0.2746 - val_accuracy: 0.8969 - val_recall: 0.8937\n",
      "Epoch 11/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9038 - recall: 0.9049\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 242us/sample - loss: 0.2618 - accuracy: 0.9034 - recall: 0.9044 - val_loss: 0.2524 - val_accuracy: 0.8983 - val_recall: 0.8986\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 110us/sample - loss: 0.2366 - accuracy: 0.9048 - recall: 0.9062\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.5235 - accuracy: 0.8701 - recall: 0.8736\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 274us/sample - loss: 0.5208 - accuracy: 0.8703 - recall: 0.8736 - val_loss: 0.3967 - val_accuracy: 0.8821 - val_recall: 0.8846\n",
      "Epoch 2/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3267 - accuracy: 0.8865 - recall: 0.8861\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.3266 - accuracy: 0.8864 - recall: 0.8862 - val_loss: 0.2785 - val_accuracy: 0.8867 - val_recall: 0.8837\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.8907 - recall: 0.8908\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.3073 - accuracy: 0.8901 - recall: 0.8902 - val_loss: 0.2736 - val_accuracy: 0.8907 - val_recall: 0.8920\n",
      "Epoch 4/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.8940 - recall: 0.8943\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 278us/sample - loss: 0.3002 - accuracy: 0.8948 - recall: 0.8951 - val_loss: 0.2789 - val_accuracy: 0.8909 - val_recall: 0.8930\n",
      "Epoch 5/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8969 - recall: 0.8964\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.2954 - accuracy: 0.8962 - recall: 0.8957 - val_loss: 0.2850 - val_accuracy: 0.8892 - val_recall: 0.8886\n",
      "Epoch 6/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.8971 - recall: 0.8972\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.2878 - accuracy: 0.8974 - recall: 0.8974 - val_loss: 1.3265 - val_accuracy: 0.6726 - val_recall: 0.6903\n",
      "Epoch 7/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8973 - recall: 0.8968\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 255us/sample - loss: 0.3026 - accuracy: 0.8971 - recall: 0.8966 - val_loss: 0.3012 - val_accuracy: 0.8849 - val_recall: 0.8849\n",
      "Epoch 8/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.9020 - recall: 0.9023\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 271us/sample - loss: 0.2700 - accuracy: 0.9021 - recall: 0.9025 - val_loss: 0.2748 - val_accuracy: 0.8977 - val_recall: 0.9024\n",
      "Epoch 9/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.8992 - recall: 0.8983\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 258us/sample - loss: 0.2758 - accuracy: 0.8993 - recall: 0.8984 - val_loss: 0.2799 - val_accuracy: 0.8949 - val_recall: 0.8936\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2786 - accuracy: 0.8997 - recall: 0.8993\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 243us/sample - loss: 0.2769 - accuracy: 0.9004 - recall: 0.8999 - val_loss: 0.2704 - val_accuracy: 0.8965 - val_recall: 0.8955\n",
      "Epoch 11/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9049 - recall: 0.9043\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2640 - accuracy: 0.9040 - recall: 0.9034 - val_loss: 0.2651 - val_accuracy: 0.9009 - val_recall: 0.9022\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 130us/sample - loss: 0.2780 - accuracy: 0.8954 - recall: 0.8960\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.5785 - accuracy: 0.8616 - recall: 0.8662\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 322us/sample - loss: 0.5742 - accuracy: 0.8619 - recall: 0.8665 - val_loss: 0.3146 - val_accuracy: 0.8797 - val_recall: 0.8810\n",
      "Epoch 2/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8811 - recall: 0.8813\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 258us/sample - loss: 0.3501 - accuracy: 0.8812 - recall: 0.8813 - val_loss: 0.3325 - val_accuracy: 0.8678 - val_recall: 0.8674\n",
      "Epoch 3/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.8869 - recall: 0.8864\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.3106 - accuracy: 0.8868 - recall: 0.8863 - val_loss: 0.3571 - val_accuracy: 0.8843 - val_recall: 0.8839\n",
      "Epoch 4/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8902 - recall: 0.8896\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.3076 - accuracy: 0.8905 - recall: 0.8899 - val_loss: 0.2925 - val_accuracy: 0.8905 - val_recall: 0.8917\n",
      "Epoch 5/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.8883 - recall: 0.8881\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 291us/sample - loss: 0.3179 - accuracy: 0.8881 - recall: 0.8880 - val_loss: 0.2933 - val_accuracy: 0.8917 - val_recall: 0.8850\n",
      "Epoch 6/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.8937 - recall: 0.8942\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.3039 - accuracy: 0.8940 - recall: 0.8946 - val_loss: 0.2891 - val_accuracy: 0.8907 - val_recall: 0.8933\n",
      "Epoch 7/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.8911 - recall: 0.8913\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 242us/sample - loss: 0.3144 - accuracy: 0.8920 - recall: 0.8923 - val_loss: 0.2905 - val_accuracy: 0.8959 - val_recall: 0.8928\n",
      "Epoch 8/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.8979 - recall: 0.8977\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 242us/sample - loss: 0.2839 - accuracy: 0.8980 - recall: 0.8978 - val_loss: 0.2757 - val_accuracy: 0.8964 - val_recall: 0.8962\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.8968 - recall: 0.8972\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 243us/sample - loss: 0.2738 - accuracy: 0.8970 - recall: 0.8974 - val_loss: 0.2756 - val_accuracy: 0.8982 - val_recall: 0.8998\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.8990 - recall: 0.8981\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.2768 - accuracy: 0.8991 - recall: 0.8981 - val_loss: 0.2728 - val_accuracy: 0.8993 - val_recall: 0.8977\n",
      "Epoch 11/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.9009 - recall: 0.9005\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 242us/sample - loss: 0.2724 - accuracy: 0.9012 - recall: 0.9009 - val_loss: 0.2799 - val_accuracy: 0.8951 - val_recall: 0.8953\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 112us/sample - loss: 0.2714 - accuracy: 0.8987 - recall: 0.8993\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.5554 - accuracy: 0.8575 - recall: 0.8574\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 276us/sample - loss: 0.5524 - accuracy: 0.8580 - recall: 0.8580 - val_loss: 0.3729 - val_accuracy: 0.8797 - val_recall: 0.8815\n",
      "Epoch 2/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3541 - accuracy: 0.8806 - recall: 0.8793\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.3538 - accuracy: 0.8803 - recall: 0.8791 - val_loss: 0.3450 - val_accuracy: 0.8654 - val_recall: 0.8777\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.8837 - recall: 0.8848\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.3155 - accuracy: 0.8842 - recall: 0.8851 - val_loss: 0.2820 - val_accuracy: 0.8850 - val_recall: 0.8850\n",
      "Epoch 4/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8871 - recall: 0.8872\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.3095 - accuracy: 0.8874 - recall: 0.8874 - val_loss: 0.2698 - val_accuracy: 0.8887 - val_recall: 0.8902\n",
      "Epoch 5/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8868 - recall: 0.8871\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.3202 - accuracy: 0.8869 - recall: 0.8873 - val_loss: 0.2925 - val_accuracy: 0.8900 - val_recall: 0.8898\n",
      "Epoch 6/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8912 - recall: 0.8921\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.3082 - accuracy: 0.8913 - recall: 0.8923 - val_loss: 0.2869 - val_accuracy: 0.8956 - val_recall: 0.8970\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.8925 - recall: 0.8933\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2917 - accuracy: 0.8924 - recall: 0.8931 - val_loss: 0.3315 - val_accuracy: 0.8958 - val_recall: 0.8942\n",
      "Epoch 8/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8908 - recall: 0.8910\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.2941 - accuracy: 0.8913 - recall: 0.8916 - val_loss: 0.2984 - val_accuracy: 0.8944 - val_recall: 0.8953\n",
      "Epoch 9/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.8989 - recall: 0.8992\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2851 - accuracy: 0.8993 - recall: 0.8996 - val_loss: 0.2706 - val_accuracy: 0.8700 - val_recall: 0.8719\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2733 - accuracy: 0.8933 - recall: 0.8940\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.2738 - accuracy: 0.8938 - recall: 0.8946 - val_loss: 0.2971 - val_accuracy: 0.8852 - val_recall: 0.8835\n",
      "Epoch 11/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.8948 - recall: 0.8949\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.2770 - accuracy: 0.8954 - recall: 0.8953 - val_loss: 0.3135 - val_accuracy: 0.8906 - val_recall: 0.8824\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 110us/sample - loss: 0.2709 - accuracy: 0.9083 - recall: 0.9020\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.5639 - accuracy: 0.8698 - recall: 0.8699\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 273us/sample - loss: 0.5566 - accuracy: 0.8705 - recall: 0.8706 - val_loss: 0.3174 - val_accuracy: 0.8847 - val_recall: 0.8884\n",
      "Epoch 2/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8901 - recall: 0.8901\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.3090 - accuracy: 0.8903 - recall: 0.8903 - val_loss: 0.2714 - val_accuracy: 0.8912 - val_recall: 0.8938\n",
      "Epoch 3/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.8946 - recall: 0.8956\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.2880 - accuracy: 0.8943 - recall: 0.8952 - val_loss: 0.3301 - val_accuracy: 0.8905 - val_recall: 0.8904\n",
      "Epoch 4/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2774 - accuracy: 0.8967 - recall: 0.8963\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.2772 - accuracy: 0.8964 - recall: 0.8963 - val_loss: 0.2580 - val_accuracy: 0.8939 - val_recall: 0.9044\n",
      "Epoch 5/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.8984 - recall: 0.8987\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.2624 - accuracy: 0.8982 - recall: 0.8984 - val_loss: 0.2786 - val_accuracy: 0.8887 - val_recall: 0.8841\n",
      "Epoch 6/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2552 - accuracy: 0.9008 - recall: 0.9006\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.2557 - accuracy: 0.9009 - recall: 0.9007 - val_loss: 0.2477 - val_accuracy: 0.8930 - val_recall: 0.8928\n",
      "Epoch 7/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2585 - accuracy: 0.9010 - recall: 0.9012\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.2582 - accuracy: 0.9005 - recall: 0.9007 - val_loss: 0.2495 - val_accuracy: 0.8926 - val_recall: 0.8922\n",
      "Epoch 8/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.9031 - recall: 0.9034\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 243us/sample - loss: 0.2550 - accuracy: 0.9032 - recall: 0.9036 - val_loss: 0.2574 - val_accuracy: 0.8909 - val_recall: 0.8913\n",
      "Epoch 9/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9033 - recall: 0.9023\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.2447 - accuracy: 0.9030 - recall: 0.9020 - val_loss: 0.2681 - val_accuracy: 0.8958 - val_recall: 0.8953\n",
      "Epoch 10/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.9019 - recall: 0.9011\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.2498 - accuracy: 0.9021 - recall: 0.9014 - val_loss: 0.2440 - val_accuracy: 0.8942 - val_recall: 0.8933\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9020 - recall: 0.9016\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 262us/sample - loss: 0.2396 - accuracy: 0.9022 - recall: 0.9018 - val_loss: 0.2509 - val_accuracy: 0.8963 - val_recall: 0.8959\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 104us/sample - loss: 0.2486 - accuracy: 0.9011 - recall: 0.9006\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.4977 - accuracy: 0.8715 - recall: 0.8686\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 310us/sample - loss: 0.4939 - accuracy: 0.8723 - recall: 0.8693 - val_loss: 0.3040 - val_accuracy: 0.8845 - val_recall: 0.8868\n",
      "Epoch 2/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8890 - recall: 0.8890\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.3339 - accuracy: 0.8890 - recall: 0.8890 - val_loss: 0.3015 - val_accuracy: 0.8869 - val_recall: 0.8847\n",
      "Epoch 3/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.2789 - accuracy: 0.8968 - recall: 0.8968\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 255us/sample - loss: 0.2812 - accuracy: 0.8955 - recall: 0.8955 - val_loss: 0.2775 - val_accuracy: 0.8900 - val_recall: 0.8893\n",
      "Epoch 4/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.8964 - recall: 0.8961\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 255us/sample - loss: 0.2867 - accuracy: 0.8963 - recall: 0.8960 - val_loss: 0.2765 - val_accuracy: 0.8945 - val_recall: 0.8941\n",
      "Epoch 5/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8984 - recall: 0.8980\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 222us/sample - loss: 0.2741 - accuracy: 0.8987 - recall: 0.8982 - val_loss: 0.2653 - val_accuracy: 0.8963 - val_recall: 0.8964\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2677 - accuracy: 0.9010 - recall: 0.9021\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 226us/sample - loss: 0.2674 - accuracy: 0.9010 - recall: 0.9021 - val_loss: 0.2661 - val_accuracy: 0.8973 - val_recall: 0.8966\n",
      "Epoch 7/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2715 - accuracy: 0.8987 - recall: 0.8987\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 222us/sample - loss: 0.2702 - accuracy: 0.9001 - recall: 0.9002 - val_loss: 0.2563 - val_accuracy: 0.8994 - val_recall: 0.9010\n",
      "Epoch 8/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2572 - accuracy: 0.9041 - recall: 0.9056\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 220us/sample - loss: 0.2579 - accuracy: 0.9037 - recall: 0.9050 - val_loss: 0.2714 - val_accuracy: 0.8964 - val_recall: 0.8968\n",
      "Epoch 9/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2625 - accuracy: 0.9030 - recall: 0.9034\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 222us/sample - loss: 0.2627 - accuracy: 0.9028 - recall: 0.9031 - val_loss: 0.2530 - val_accuracy: 0.9020 - val_recall: 0.9022\n",
      "Epoch 10/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.9054 - recall: 0.9059\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 220us/sample - loss: 0.2513 - accuracy: 0.9051 - recall: 0.9056 - val_loss: 0.2523 - val_accuracy: 0.8999 - val_recall: 0.8994\n",
      "Epoch 11/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2495 - accuracy: 0.9046 - recall: 0.9047\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 219us/sample - loss: 0.2496 - accuracy: 0.9045 - recall: 0.9046 - val_loss: 0.2405 - val_accuracy: 0.9017 - val_recall: 0.9022\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 99us/sample - loss: 0.2358 - accuracy: 0.9067 - recall: 0.9082\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.5387 - accuracy: 0.8720 - recall: 0.8717\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.5345 - accuracy: 0.8721 - recall: 0.8721 - val_loss: 0.3316 - val_accuracy: 0.8779 - val_recall: 0.9011\n",
      "Epoch 2/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8942 - recall: 0.8937\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 218us/sample - loss: 0.3429 - accuracy: 0.8943 - recall: 0.8937 - val_loss: 0.2866 - val_accuracy: 0.8899 - val_recall: 0.8868\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8960 - recall: 0.8968\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 223us/sample - loss: 0.3022 - accuracy: 0.8966 - recall: 0.8973 - val_loss: 0.2913 - val_accuracy: 0.8873 - val_recall: 0.8870\n",
      "Epoch 4/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2702 - accuracy: 0.8984 - recall: 0.8974\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 227us/sample - loss: 0.2723 - accuracy: 0.8975 - recall: 0.8968 - val_loss: 0.3342 - val_accuracy: 0.8744 - val_recall: 0.8813\n",
      "Epoch 5/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2625 - accuracy: 0.9017 - recall: 0.9016\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 230us/sample - loss: 0.2621 - accuracy: 0.9018 - recall: 0.9017 - val_loss: 0.2712 - val_accuracy: 0.8944 - val_recall: 0.8955\n",
      "Epoch 6/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2586 - accuracy: 0.9023 - recall: 0.9026\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.2565 - accuracy: 0.9033 - recall: 0.9036 - val_loss: 0.2579 - val_accuracy: 0.8992 - val_recall: 0.8997\n",
      "Epoch 7/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2478 - accuracy: 0.9036 - recall: 0.9042\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2477 - accuracy: 0.9033 - recall: 0.9039 - val_loss: 0.2498 - val_accuracy: 0.8980 - val_recall: 0.8973\n",
      "Epoch 8/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9045 - recall: 0.9041\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 233us/sample - loss: 0.2521 - accuracy: 0.9043 - recall: 0.9039 - val_loss: 0.2616 - val_accuracy: 0.8977 - val_recall: 0.8978\n",
      "Epoch 9/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.9041 - recall: 0.9043\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 226us/sample - loss: 0.2603 - accuracy: 0.9044 - recall: 0.9045 - val_loss: 0.2871 - val_accuracy: 0.8943 - val_recall: 0.8940\n",
      "Epoch 10/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2450 - accuracy: 0.9045 - recall: 0.9034\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 225us/sample - loss: 0.2435 - accuracy: 0.9050 - recall: 0.9039 - val_loss: 0.2561 - val_accuracy: 0.9002 - val_recall: 0.8997\n",
      "Epoch 11/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2424 - accuracy: 0.9065 - recall: 0.9061\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 225us/sample - loss: 0.2417 - accuracy: 0.9072 - recall: 0.9069 - val_loss: 0.2557 - val_accuracy: 0.9006 - val_recall: 0.9000\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 107us/sample - loss: 0.2711 - accuracy: 0.8974 - recall: 0.8964\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.5024 - accuracy: 0.8704 - recall: 0.8707\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 4s 382us/sample - loss: 0.5007 - accuracy: 0.8700 - recall: 0.8705 - val_loss: 0.3085 - val_accuracy: 0.8822 - val_recall: 0.8699\n",
      "Epoch 2/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3000 - accuracy: 0.8920 - recall: 0.8921\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2989 - accuracy: 0.8924 - recall: 0.8925 - val_loss: 0.2706 - val_accuracy: 0.8714 - val_recall: 0.8720\n",
      "Epoch 3/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2765 - accuracy: 0.8917 - recall: 0.8914\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 243us/sample - loss: 0.2770 - accuracy: 0.8915 - recall: 0.8912 - val_loss: 0.2872 - val_accuracy: 0.8862 - val_recall: 0.8864\n",
      "Epoch 4/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.8943 - recall: 0.8945\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 224us/sample - loss: 0.2762 - accuracy: 0.8944 - recall: 0.8946 - val_loss: 0.2644 - val_accuracy: 0.8927 - val_recall: 0.8924\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2550 - accuracy: 0.8987 - recall: 0.8993\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 222us/sample - loss: 0.2541 - accuracy: 0.8988 - recall: 0.8993 - val_loss: 0.2541 - val_accuracy: 0.8947 - val_recall: 0.8949\n",
      "Epoch 6/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.9011 - recall: 0.9016\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 232us/sample - loss: 0.2477 - accuracy: 0.9013 - recall: 0.9018 - val_loss: 0.2473 - val_accuracy: 0.8957 - val_recall: 0.8961\n",
      "Epoch 7/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.9011 - recall: 0.9014\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 237us/sample - loss: 0.2547 - accuracy: 0.9009 - recall: 0.9013 - val_loss: 0.2477 - val_accuracy: 0.8986 - val_recall: 0.8986\n",
      "Epoch 8/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2465 - accuracy: 0.9016 - recall: 0.9018\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.2462 - accuracy: 0.9015 - recall: 0.9017 - val_loss: 0.2471 - val_accuracy: 0.8986 - val_recall: 0.8986\n",
      "Epoch 9/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.9045 - recall: 0.9046\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 234us/sample - loss: 0.2389 - accuracy: 0.9039 - recall: 0.9039 - val_loss: 0.2520 - val_accuracy: 0.8975 - val_recall: 0.8975\n",
      "Epoch 10/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.9022 - recall: 0.9022\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 233us/sample - loss: 0.2388 - accuracy: 0.9025 - recall: 0.9024 - val_loss: 0.2412 - val_accuracy: 0.9004 - val_recall: 0.8960\n",
      "Epoch 11/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9046 - recall: 0.9038\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 239us/sample - loss: 0.2387 - accuracy: 0.9048 - recall: 0.9040 - val_loss: 0.2348 - val_accuracy: 0.9022 - val_recall: 0.9011\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 112us/sample - loss: 0.2336 - accuracy: 0.9077 - recall: 0.9078\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.5571 - accuracy: 0.8625 - recall: 0.8648\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 304us/sample - loss: 0.5526 - accuracy: 0.8632 - recall: 0.8654 - val_loss: 0.2941 - val_accuracy: 0.8796 - val_recall: 0.8813\n",
      "Epoch 2/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8869 - recall: 0.8887\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.3049 - accuracy: 0.8869 - recall: 0.8887 - val_loss: 0.2919 - val_accuracy: 0.8783 - val_recall: 0.8808\n",
      "Epoch 3/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8905 - recall: 0.8913\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 230us/sample - loss: 0.2959 - accuracy: 0.8906 - recall: 0.8915 - val_loss: 0.2650 - val_accuracy: 0.8800 - val_recall: 0.8815\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8955 - recall: 0.8962\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 237us/sample - loss: 0.2971 - accuracy: 0.8951 - recall: 0.8958 - val_loss: 0.2795 - val_accuracy: 0.8968 - val_recall: 0.8964\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.8956 - recall: 0.8964\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 227us/sample - loss: 0.2762 - accuracy: 0.8946 - recall: 0.8955 - val_loss: 0.2668 - val_accuracy: 0.8903 - val_recall: 0.8897\n",
      "Epoch 6/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.8962 - recall: 0.8970- ETA: 0s - loss: 0.2648 - accuracy: 0.8972 -\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 228us/sample - loss: 0.2700 - accuracy: 0.8969 - recall: 0.8977 - val_loss: 0.2585 - val_accuracy: 0.8955 - val_recall: 0.8942\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2591 - accuracy: 0.8991 - recall: 0.8989\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 227us/sample - loss: 0.2600 - accuracy: 0.8990 - recall: 0.8988 - val_loss: 0.2597 - val_accuracy: 0.8927 - val_recall: 0.8924\n",
      "Epoch 8/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2591 - accuracy: 0.8981 - recall: 0.8986\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 240us/sample - loss: 0.2597 - accuracy: 0.8977 - recall: 0.8981 - val_loss: 0.2487 - val_accuracy: 0.8948 - val_recall: 0.8947\n",
      "Epoch 9/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.9011 - recall: 0.9018\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 227us/sample - loss: 0.2514 - accuracy: 0.9012 - recall: 0.9019 - val_loss: 0.2546 - val_accuracy: 0.8975 - val_recall: 0.8977\n",
      "Epoch 10/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.9019 - recall: 0.9019\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 258us/sample - loss: 0.2490 - accuracy: 0.9022 - recall: 0.9022 - val_loss: 0.2401 - val_accuracy: 0.8995 - val_recall: 0.8999\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2427 - accuracy: 0.9043 - recall: 0.9044\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 311us/sample - loss: 0.2426 - accuracy: 0.9045 - recall: 0.9046 - val_loss: 0.2439 - val_accuracy: 0.9020 - val_recall: 0.9012\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 148us/sample - loss: 0.2188 - accuracy: 0.9131 - recall: 0.9128\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 1.0105 - accuracy: 0.8632 - recall: 0.8712\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 329us/sample - loss: 1.0091 - accuracy: 0.8634 - recall: 0.8715 - val_loss: 0.6493 - val_accuracy: 0.8707 - val_recall: 0.8980\n",
      "Epoch 2/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.4984 - accuracy: 0.8704 - recall: 0.8675\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 285us/sample - loss: 0.4957 - accuracy: 0.8707 - recall: 0.8677 - val_loss: 0.3442 - val_accuracy: 0.8852 - val_recall: 0.8810\n",
      "Epoch 3/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8834 - recall: 0.8830\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.3473 - accuracy: 0.8834 - recall: 0.8829 - val_loss: 0.3088 - val_accuracy: 0.8854 - val_recall: 0.8844\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8885 - recall: 0.8882- ETA: 0s - loss: 0.3265 - accuracy: 0.8839 - r\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 280us/sample - loss: 0.3092 - accuracy: 0.8889 - recall: 0.8886 - val_loss: 0.2758 - val_accuracy: 0.8879 - val_recall: 0.8839\n",
      "Epoch 5/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.8916 - recall: 0.8902\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 255us/sample - loss: 0.2898 - accuracy: 0.8919 - recall: 0.8906 - val_loss: 0.2893 - val_accuracy: 0.8887 - val_recall: 0.8871\n",
      "Epoch 6/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.8968 - recall: 0.8946\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2806 - accuracy: 0.8968 - recall: 0.8946 - val_loss: 0.2869 - val_accuracy: 0.8921 - val_recall: 0.8895\n",
      "Epoch 7/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8953 - recall: 0.8926\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.2735 - accuracy: 0.8950 - recall: 0.8923 - val_loss: 0.3002 - val_accuracy: 0.8903 - val_recall: 0.8899\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.8975 - recall: 0.8971\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.2674 - accuracy: 0.8978 - recall: 0.8974 - val_loss: 0.2784 - val_accuracy: 0.8940 - val_recall: 0.8949\n",
      "Epoch 9/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.8990 - recall: 0.8992\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.2671 - accuracy: 0.8987 - recall: 0.8988 - val_loss: 0.2849 - val_accuracy: 0.8939 - val_recall: 0.8939\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2636 - accuracy: 0.8992 - recall: 0.9003\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.2635 - accuracy: 0.8994 - recall: 0.9005 - val_loss: 0.2804 - val_accuracy: 0.8952 - val_recall: 0.8953\n",
      "Epoch 11/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.8996 - recall: 0.9004\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.2555 - accuracy: 0.8991 - recall: 0.9000 - val_loss: 0.2800 - val_accuracy: 0.8914 - val_recall: 0.8975\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 105us/sample - loss: 0.2913 - accuracy: 0.8925 - recall: 0.8987\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.9548 - accuracy: 0.8587 - recall: 0.8550\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 281us/sample - loss: 0.9468 - accuracy: 0.8591 - recall: 0.8556 - val_loss: 0.4566 - val_accuracy: 0.8708 - val_recall: 0.8999\n",
      "Epoch 2/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8698 - recall: 0.8692\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.4171 - accuracy: 0.8697 - recall: 0.8691 - val_loss: 0.3071 - val_accuracy: 0.8763 - val_recall: 0.8838\n",
      "Epoch 3/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8819 - recall: 0.8795\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.3305 - accuracy: 0.8819 - recall: 0.8795 - val_loss: 0.2843 - val_accuracy: 0.8840 - val_recall: 0.8869\n",
      "Epoch 4/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.8824 - recall: 0.8867\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 249us/sample - loss: 0.3176 - accuracy: 0.8826 - recall: 0.8868 - val_loss: 0.2797 - val_accuracy: 0.8861 - val_recall: 0.8866\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8897 - recall: 0.8902\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 249us/sample - loss: 0.2933 - accuracy: 0.8898 - recall: 0.8902 - val_loss: 0.2665 - val_accuracy: 0.8881 - val_recall: 0.8886\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.8937 - recall: 0.8948\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.2885 - accuracy: 0.8937 - recall: 0.8947 - val_loss: 0.2733 - val_accuracy: 0.8928 - val_recall: 0.8931\n",
      "Epoch 7/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8949 - recall: 0.8954\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2807 - accuracy: 0.8949 - recall: 0.8954 - val_loss: 0.2659 - val_accuracy: 0.8951 - val_recall: 0.8957\n",
      "Epoch 8/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2673 - accuracy: 0.8982 - recall: 0.8985\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 249us/sample - loss: 0.2691 - accuracy: 0.8977 - recall: 0.8980 - val_loss: 0.2810 - val_accuracy: 0.8935 - val_recall: 0.8937\n",
      "Epoch 9/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.8989 - recall: 0.8992\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2610 - accuracy: 0.8990 - recall: 0.8993 - val_loss: 0.2794 - val_accuracy: 0.8949 - val_recall: 0.8949\n",
      "Epoch 10/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2591 - accuracy: 0.8990 - recall: 0.8999\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.2587 - accuracy: 0.8992 - recall: 0.9001 - val_loss: 0.2517 - val_accuracy: 0.8953 - val_recall: 0.8957\n",
      "Epoch 11/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.9000 - recall: 0.9003- ETA: 1s - loss: 0.2461 - a\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.2555 - accuracy: 0.9007 - recall: 0.9010 - val_loss: 0.2537 - val_accuracy: 0.8959 - val_recall: 0.8962\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 105us/sample - loss: 0.2389 - accuracy: 0.9055 - recall: 0.9070\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 1.0040 - accuracy: 0.8585 - recall: 0.8585\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 290us/sample - loss: 0.9955 - accuracy: 0.8587 - recall: 0.8590 - val_loss: 0.5027 - val_accuracy: 0.8642 - val_recall: 0.8813\n",
      "Epoch 2/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.4956 - accuracy: 0.8706 - recall: 0.8701\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 280us/sample - loss: 0.4956 - accuracy: 0.8706 - recall: 0.8700 - val_loss: 0.3590 - val_accuracy: 0.8549 - val_recall: 0.7994\n",
      "Epoch 3/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8866 - recall: 0.8867\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 300us/sample - loss: 0.3436 - accuracy: 0.8871 - recall: 0.8875 - val_loss: 0.3047 - val_accuracy: 0.8831 - val_recall: 0.8826\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3028 - accuracy: 0.8918 - recall: 0.8917\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 271us/sample - loss: 0.3032 - accuracy: 0.8922 - recall: 0.8922 - val_loss: 0.3054 - val_accuracy: 0.8903 - val_recall: 0.8922\n",
      "Epoch 5/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8969 - recall: 0.8975\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 279us/sample - loss: 0.2877 - accuracy: 0.8969 - recall: 0.8975 - val_loss: 0.2655 - val_accuracy: 0.8899 - val_recall: 0.8914\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.8989 - recall: 0.8981\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 299us/sample - loss: 0.2787 - accuracy: 0.8988 - recall: 0.8980 - val_loss: 0.2611 - val_accuracy: 0.8929 - val_recall: 0.8937\n",
      "Epoch 7/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2694 - accuracy: 0.8993 - recall: 0.8987\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 296us/sample - loss: 0.2692 - accuracy: 0.8999 - recall: 0.8992 - val_loss: 0.2586 - val_accuracy: 0.8922 - val_recall: 0.8918\n",
      "Epoch 8/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2640 - accuracy: 0.8984 - recall: 0.8979\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 286us/sample - loss: 0.2625 - accuracy: 0.8990 - recall: 0.8984 - val_loss: 0.2533 - val_accuracy: 0.8922 - val_recall: 0.8908\n",
      "Epoch 9/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2582 - accuracy: 0.9002 - recall: 0.8983\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 316us/sample - loss: 0.2568 - accuracy: 0.9011 - recall: 0.8992 - val_loss: 0.2525 - val_accuracy: 0.8951 - val_recall: 0.8935\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2549 - accuracy: 0.9041 - recall: 0.9032\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 312us/sample - loss: 0.2545 - accuracy: 0.9040 - recall: 0.9031 - val_loss: 0.2528 - val_accuracy: 0.8942 - val_recall: 0.8939\n",
      "Epoch 11/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2489 - accuracy: 0.9045 - recall: 0.9038\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 323us/sample - loss: 0.2494 - accuracy: 0.9042 - recall: 0.9036 - val_loss: 0.2968 - val_accuracy: 0.8939 - val_recall: 0.8946\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 126us/sample - loss: 0.2998 - accuracy: 0.8921 - recall: 0.8925\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.9548 - accuracy: 0.8566 - recall: 0.8566\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 4s 351us/sample - loss: 0.9517 - accuracy: 0.8569 - recall: 0.8570 - val_loss: 0.4534 - val_accuracy: 0.8640 - val_recall: 0.8382\n",
      "Epoch 2/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3992 - accuracy: 0.8749 - recall: 0.8733\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 290us/sample - loss: 0.3988 - accuracy: 0.8750 - recall: 0.8735 - val_loss: 0.3386 - val_accuracy: 0.8838 - val_recall: 0.8828\n",
      "Epoch 3/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8851 - recall: 0.8827\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 322us/sample - loss: 0.3314 - accuracy: 0.8852 - recall: 0.8829 - val_loss: 0.2939 - val_accuracy: 0.8876 - val_recall: 0.8888\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8897 - recall: 0.8894\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 317us/sample - loss: 0.2950 - accuracy: 0.8901 - recall: 0.8898 - val_loss: 0.2788 - val_accuracy: 0.8919 - val_recall: 0.8920\n",
      "Epoch 5/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.8919 - recall: 0.8913\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 308us/sample - loss: 0.2838 - accuracy: 0.8920 - recall: 0.8914 - val_loss: 0.2763 - val_accuracy: 0.8856 - val_recall: 0.8844\n",
      "Epoch 6/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2671 - accuracy: 0.8951 - recall: 0.8950\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 323us/sample - loss: 0.2673 - accuracy: 0.8947 - recall: 0.8946 - val_loss: 0.2711 - val_accuracy: 0.8950 - val_recall: 0.8935\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8955 - recall: 0.8953\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 290us/sample - loss: 0.2638 - accuracy: 0.8954 - recall: 0.8951 - val_loss: 0.2905 - val_accuracy: 0.8948 - val_recall: 0.8937\n",
      "Epoch 8/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2629 - accuracy: 0.8966 - recall: 0.8972\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 305us/sample - loss: 0.2631 - accuracy: 0.8965 - recall: 0.8971 - val_loss: 0.2639 - val_accuracy: 0.8922 - val_recall: 0.8939\n",
      "Epoch 9/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.8993 - recall: 0.9001\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 296us/sample - loss: 0.2526 - accuracy: 0.8999 - recall: 0.9007 - val_loss: 0.2512 - val_accuracy: 0.8970 - val_recall: 0.8968\n",
      "Epoch 10/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.8990 - recall: 0.8989\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 289us/sample - loss: 0.2533 - accuracy: 0.8992 - recall: 0.8991 - val_loss: 0.2529 - val_accuracy: 0.8945 - val_recall: 0.8949\n",
      "Epoch 11/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.8982 - recall: 0.8985\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 287us/sample - loss: 0.2537 - accuracy: 0.8982 - recall: 0.8985 - val_loss: 0.2479 - val_accuracy: 0.8970 - val_recall: 0.8966\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 107us/sample - loss: 0.2406 - accuracy: 0.9067 - recall: 0.9074\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.8884 - accuracy: 0.8513 - recall: 0.8506\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 287us/sample - loss: 0.8827 - accuracy: 0.8510 - recall: 0.8503 - val_loss: 0.3905 - val_accuracy: 0.8621 - val_recall: 0.8939\n",
      "Epoch 2/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.4484 - accuracy: 0.8685 - recall: 0.8692\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.4492 - accuracy: 0.8680 - recall: 0.8691 - val_loss: 0.3343 - val_accuracy: 0.8669 - val_recall: 0.8597\n",
      "Epoch 3/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3440 - accuracy: 0.8798 - recall: 0.8755\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.3438 - accuracy: 0.8799 - recall: 0.8755 - val_loss: 0.2923 - val_accuracy: 0.8886 - val_recall: 0.8901\n",
      "Epoch 4/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8818 - recall: 0.8845\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 274us/sample - loss: 0.3094 - accuracy: 0.8823 - recall: 0.8850 - val_loss: 0.2944 - val_accuracy: 0.8916 - val_recall: 0.8951\n",
      "Epoch 5/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8872 - recall: 0.8885\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 286us/sample - loss: 0.2942 - accuracy: 0.8876 - recall: 0.8889 - val_loss: 0.2689 - val_accuracy: 0.8935 - val_recall: 0.8946\n",
      "Epoch 6/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.8893 - recall: 0.8896\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 282us/sample - loss: 0.2789 - accuracy: 0.8894 - recall: 0.8896 - val_loss: 0.2780 - val_accuracy: 0.8838 - val_recall: 0.8833\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2729 - accuracy: 0.8927 - recall: 0.8929\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 295us/sample - loss: 0.2727 - accuracy: 0.8928 - recall: 0.8930 - val_loss: 0.3123 - val_accuracy: 0.8922 - val_recall: 0.8924\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.8947 - recall: 0.8943\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 276us/sample - loss: 0.2704 - accuracy: 0.8945 - recall: 0.8942 - val_loss: 0.2593 - val_accuracy: 0.8910 - val_recall: 0.8895\n",
      "Epoch 9/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.8968 - recall: 0.8966\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 295us/sample - loss: 0.2655 - accuracy: 0.8966 - recall: 0.8963 - val_loss: 0.3048 - val_accuracy: 0.8952 - val_recall: 0.8940\n",
      "Epoch 10/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2579 - accuracy: 0.8969 - recall: 0.8957\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 265us/sample - loss: 0.2574 - accuracy: 0.8969 - recall: 0.8957 - val_loss: 0.2472 - val_accuracy: 0.8957 - val_recall: 0.8951\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.8990 - recall: 0.8999\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 280us/sample - loss: 0.2589 - accuracy: 0.8993 - recall: 0.9001 - val_loss: 0.2582 - val_accuracy: 0.8970 - val_recall: 0.8966\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 115us/sample - loss: 0.2342 - accuracy: 0.9135 - recall: 0.9140\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.8176 - accuracy: 0.8512 - recall: 0.8525\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 287us/sample - loss: 0.8149 - accuracy: 0.8501 - recall: 0.8507 - val_loss: 0.3823 - val_accuracy: 0.8352 - val_recall: 0.7950\n",
      "Epoch 2/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.4724 - accuracy: 0.8604 - recall: 0.8551\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 225us/sample - loss: 0.4725 - accuracy: 0.8605 - recall: 0.8555 - val_loss: 0.3562 - val_accuracy: 0.8774 - val_recall: 0.8902\n",
      "Epoch 3/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8723 - recall: 0.8724\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 279us/sample - loss: 0.3625 - accuracy: 0.8720 - recall: 0.8721 - val_loss: 0.3220 - val_accuracy: 0.8834 - val_recall: 0.8889\n",
      "Epoch 4/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3444 - accuracy: 0.8839 - recall: 0.8849\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 265us/sample - loss: 0.3439 - accuracy: 0.8835 - recall: 0.8845 - val_loss: 0.3063 - val_accuracy: 0.8842 - val_recall: 0.8826\n",
      "Epoch 5/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8847 - recall: 0.8831\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.3255 - accuracy: 0.8843 - recall: 0.8826 - val_loss: 0.3036 - val_accuracy: 0.8848 - val_recall: 0.8839\n",
      "Epoch 6/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.8902 - recall: 0.8910\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 238us/sample - loss: 0.3041 - accuracy: 0.8896 - recall: 0.8904 - val_loss: 0.3162 - val_accuracy: 0.8892 - val_recall: 0.8888\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8905 - recall: 0.8908\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 252us/sample - loss: 0.2976 - accuracy: 0.8904 - recall: 0.8906 - val_loss: 0.2666 - val_accuracy: 0.8908 - val_recall: 0.8942\n",
      "Epoch 8/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8939 - recall: 0.8940\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 222us/sample - loss: 0.2932 - accuracy: 0.8934 - recall: 0.8935 - val_loss: 0.2876 - val_accuracy: 0.8916 - val_recall: 0.8957\n",
      "Epoch 9/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2773 - accuracy: 0.8943 - recall: 0.8941\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 221us/sample - loss: 0.2772 - accuracy: 0.8945 - recall: 0.8943 - val_loss: 0.2638 - val_accuracy: 0.8922 - val_recall: 0.8940\n",
      "Epoch 10/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8960 - recall: 0.8952\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 222us/sample - loss: 0.2786 - accuracy: 0.8962 - recall: 0.8955 - val_loss: 0.3083 - val_accuracy: 0.8930 - val_recall: 0.8986\n",
      "Epoch 11/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2809 - accuracy: 0.8977 - recall: 0.8971\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 231us/sample - loss: 0.2816 - accuracy: 0.8972 - recall: 0.8965 - val_loss: 0.2750 - val_accuracy: 0.8934 - val_recall: 0.8958\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 113us/sample - loss: 0.2722 - accuracy: 0.8966 - recall: 0.8987\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.8787 - accuracy: 0.8576 - recall: 0.8541\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 269us/sample - loss: 0.8762 - accuracy: 0.8573 - recall: 0.8541 - val_loss: 0.5080 - val_accuracy: 0.8608 - val_recall: 0.9173\n",
      "Epoch 2/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.5157 - accuracy: 0.8674 - recall: 0.8729\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 293us/sample - loss: 0.5152 - accuracy: 0.8673 - recall: 0.8727 - val_loss: 0.3378 - val_accuracy: 0.8733 - val_recall: 0.8844\n",
      "Epoch 3/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.8706 - recall: 0.8691\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 284us/sample - loss: 0.3862 - accuracy: 0.8710 - recall: 0.8695 - val_loss: 0.3091 - val_accuracy: 0.8842 - val_recall: 0.8824\n",
      "Epoch 4/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3530 - accuracy: 0.8843 - recall: 0.8816\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 305us/sample - loss: 0.3515 - accuracy: 0.8849 - recall: 0.8823 - val_loss: 0.3267 - val_accuracy: 0.8850 - val_recall: 0.8884\n",
      "Epoch 5/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.8907 - recall: 0.8911\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 305us/sample - loss: 0.3299 - accuracy: 0.8894 - recall: 0.8896 - val_loss: 0.2863 - val_accuracy: 0.8898 - val_recall: 0.8920\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8917 - recall: 0.8919\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 309us/sample - loss: 0.3183 - accuracy: 0.8916 - recall: 0.8918 - val_loss: 0.2861 - val_accuracy: 0.8888 - val_recall: 0.8920\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8931 - recall: 0.8954- ETA: 2s\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 4s 365us/sample - loss: 0.3073 - accuracy: 0.8929 - recall: 0.8952 - val_loss: 0.2819 - val_accuracy: 0.8922 - val_recall: 0.8929\n",
      "Epoch 8/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.8922 - recall: 0.8939- ETA: 0s - loss: 0.2991 - accuracy: 0.8927 - reca\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 324us/sample - loss: 0.3010 - accuracy: 0.8921 - recall: 0.8937 - val_loss: 0.2913 - val_accuracy: 0.8917 - val_recall: 0.8955\n",
      "Epoch 9/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8921 - recall: 0.8944\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 282us/sample - loss: 0.2945 - accuracy: 0.8925 - recall: 0.8949 - val_loss: 0.2740 - val_accuracy: 0.8907 - val_recall: 0.8924\n",
      "Epoch 10/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.8951 - recall: 0.8969\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.2864 - accuracy: 0.8954 - recall: 0.8972 - val_loss: 0.3084 - val_accuracy: 0.8923 - val_recall: 0.8964\n",
      "Epoch 11/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2781 - accuracy: 0.8972 - recall: 0.8989\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2781 - accuracy: 0.8969 - recall: 0.8986 - val_loss: 0.2701 - val_accuracy: 0.8932 - val_recall: 0.8927\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 113us/sample - loss: 0.2612 - accuracy: 0.9020 - recall: 0.9028\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.9201 - accuracy: 0.8550 - recall: 0.8556\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 261us/sample - loss: 0.9176 - accuracy: 0.8554 - recall: 0.8564 - val_loss: 0.4654 - val_accuracy: 0.8402 - val_recall: 0.9235\n",
      "Epoch 2/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.5347 - accuracy: 0.8601 - recall: 0.8621\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 229us/sample - loss: 0.5340 - accuracy: 0.8601 - recall: 0.8620 - val_loss: 0.4196 - val_accuracy: 0.8061 - val_recall: 0.7252\n",
      "Epoch 3/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.8671 - recall: 0.8753\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 220us/sample - loss: 0.3956 - accuracy: 0.8674 - recall: 0.8755 - val_loss: 0.3223 - val_accuracy: 0.8787 - val_recall: 0.8997\n",
      "Epoch 4/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.8798 - recall: 0.8848\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 220us/sample - loss: 0.3661 - accuracy: 0.8797 - recall: 0.8846 - val_loss: 0.2887 - val_accuracy: 0.8827 - val_recall: 0.8898\n",
      "Epoch 5/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.8856 - recall: 0.8906\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 236us/sample - loss: 0.3511 - accuracy: 0.8857 - recall: 0.8905 - val_loss: 0.3137 - val_accuracy: 0.8891 - val_recall: 0.8913\n",
      "Epoch 6/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8901 - recall: 0.8927\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 229us/sample - loss: 0.3177 - accuracy: 0.8905 - recall: 0.8931 - val_loss: 0.2738 - val_accuracy: 0.8907 - val_recall: 0.8920\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8907 - recall: 0.8914\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2991 - accuracy: 0.8908 - recall: 0.8915 - val_loss: 0.2842 - val_accuracy: 0.8919 - val_recall: 0.8917\n",
      "Epoch 8/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8954 - recall: 0.8951\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 242us/sample - loss: 0.2948 - accuracy: 0.8961 - recall: 0.8959 - val_loss: 0.2762 - val_accuracy: 0.8935 - val_recall: 0.8933\n",
      "Epoch 9/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.8961 - recall: 0.8937\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 222us/sample - loss: 0.2835 - accuracy: 0.8968 - recall: 0.8944 - val_loss: 0.2712 - val_accuracy: 0.8939 - val_recall: 0.8962\n",
      "Epoch 10/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.8931 - recall: 0.8960\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 226us/sample - loss: 0.2865 - accuracy: 0.8933 - recall: 0.8963 - val_loss: 0.2784 - val_accuracy: 0.8947 - val_recall: 0.8979\n",
      "Epoch 11/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2772 - accuracy: 0.8964 - recall: 0.8991\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.2763 - accuracy: 0.8968 - recall: 0.8995 - val_loss: 0.2749 - val_accuracy: 0.8908 - val_recall: 0.8970\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 118us/sample - loss: 0.2870 - accuracy: 0.8851 - recall: 0.8914\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.9648 - accuracy: 0.8562 - recall: 0.8551\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 295us/sample - loss: 0.9552 - accuracy: 0.8561 - recall: 0.8552 - val_loss: 0.6328 - val_accuracy: 0.8304 - val_recall: 0.8266\n",
      "Epoch 2/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.5490 - accuracy: 0.8583 - recall: 0.8624\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 233us/sample - loss: 0.5469 - accuracy: 0.8585 - recall: 0.8629 - val_loss: 0.3574 - val_accuracy: 0.8789 - val_recall: 0.8897\n",
      "Epoch 3/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8694 - recall: 0.8760\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 235us/sample - loss: 0.4076 - accuracy: 0.8697 - recall: 0.8763 - val_loss: 0.3768 - val_accuracy: 0.8817 - val_recall: 0.8884\n",
      "Epoch 4/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3614 - accuracy: 0.8797 - recall: 0.8824- ETA: 2s - loss: 0.2940\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 272us/sample - loss: 0.3612 - accuracy: 0.8795 - recall: 0.8821 - val_loss: 0.3165 - val_accuracy: 0.8859 - val_recall: 0.8810\n",
      "Epoch 5/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8770 - recall: 0.8781\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 230us/sample - loss: 0.3492 - accuracy: 0.8772 - recall: 0.8783 - val_loss: 0.2934 - val_accuracy: 0.8856 - val_recall: 0.8918\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.8846 - recall: 0.8870\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 234us/sample - loss: 0.3304 - accuracy: 0.8848 - recall: 0.8872 - val_loss: 0.2829 - val_accuracy: 0.8900 - val_recall: 0.8877\n",
      "Epoch 7/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8844 - recall: 0.8856\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.3169 - accuracy: 0.8846 - recall: 0.8858 - val_loss: 0.2786 - val_accuracy: 0.8893 - val_recall: 0.8915\n",
      "Epoch 8/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8896 - recall: 0.8932\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.3096 - accuracy: 0.8891 - recall: 0.8928 - val_loss: 0.2706 - val_accuracy: 0.8903 - val_recall: 0.8922\n",
      "Epoch 9/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8918 - recall: 0.8921\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.2908 - accuracy: 0.8919 - recall: 0.8923 - val_loss: 0.2904 - val_accuracy: 0.8918 - val_recall: 0.8924\n",
      "Epoch 10/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8924 - recall: 0.8945\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2901 - accuracy: 0.8927 - recall: 0.8948 - val_loss: 0.2753 - val_accuracy: 0.8928 - val_recall: 0.8932\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8930 - recall: 0.8944\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2855 - accuracy: 0.8930 - recall: 0.8944 - val_loss: 0.2774 - val_accuracy: 0.8954 - val_recall: 0.8953\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 101us/sample - loss: 0.2716 - accuracy: 0.9067 - recall: 0.9074\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.8550 - accuracy: 0.8487 - recall: 0.8504\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 258us/sample - loss: 0.8461 - accuracy: 0.8487 - recall: 0.8501 - val_loss: 0.3999 - val_accuracy: 0.8788 - val_recall: 0.8831\n",
      "Epoch 2/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.4588 - accuracy: 0.8613 - recall: 0.8628\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 221us/sample - loss: 0.4576 - accuracy: 0.8617 - recall: 0.8632 - val_loss: 0.3878 - val_accuracy: 0.8795 - val_recall: 0.8575\n",
      "Epoch 3/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.3952 - accuracy: 0.8731 - recall: 0.8661\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 289us/sample - loss: 0.3951 - accuracy: 0.8731 - recall: 0.8662 - val_loss: 0.3654 - val_accuracy: 0.8837 - val_recall: 0.8832\n",
      "Epoch 4/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3635 - accuracy: 0.8776 - recall: 0.8761\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 223us/sample - loss: 0.3638 - accuracy: 0.8775 - recall: 0.8760 - val_loss: 0.3436 - val_accuracy: 0.8825 - val_recall: 0.8884\n",
      "Epoch 5/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8806 - recall: 0.8808\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 227us/sample - loss: 0.3423 - accuracy: 0.8807 - recall: 0.8808 - val_loss: 0.3153 - val_accuracy: 0.8875 - val_recall: 0.8851\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3124 - accuracy: 0.8846 - recall: 0.8847\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 254us/sample - loss: 0.3127 - accuracy: 0.8845 - recall: 0.8845 - val_loss: 0.2946 - val_accuracy: 0.8894 - val_recall: 0.8875\n",
      "Epoch 7/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3366 - accuracy: 0.8865 - recall: 0.8870\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 232us/sample - loss: 0.3359 - accuracy: 0.8866 - recall: 0.8870 - val_loss: 0.3096 - val_accuracy: 0.8889 - val_recall: 0.8873\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8881 - recall: 0.8876\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 228us/sample - loss: 0.3003 - accuracy: 0.8881 - recall: 0.8878 - val_loss: 0.2820 - val_accuracy: 0.8918 - val_recall: 0.8895\n",
      "Epoch 9/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8899 - recall: 0.8876\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 227us/sample - loss: 0.3069 - accuracy: 0.8895 - recall: 0.8873 - val_loss: 0.2881 - val_accuracy: 0.8919 - val_recall: 0.8913\n",
      "Epoch 10/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.8906 - recall: 0.8891\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 4s 354us/sample - loss: 0.2867 - accuracy: 0.8909 - recall: 0.8895 - val_loss: 0.2712 - val_accuracy: 0.8924 - val_recall: 0.8933\n",
      "Epoch 11/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.8922 - recall: 0.8934\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.2838 - accuracy: 0.8919 - recall: 0.8931 - val_loss: 0.2695 - val_accuracy: 0.8941 - val_recall: 0.8926\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 109us/sample - loss: 0.2413 - accuracy: 0.9131 - recall: 0.9117\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 1.0187 - accuracy: 0.8651 - recall: 0.8581\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 321us/sample - loss: 1.0161 - accuracy: 0.8654 - recall: 0.8584 - val_loss: 0.4868 - val_accuracy: 0.8710 - val_recall: 0.8601\n",
      "Epoch 2/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.8681 - recall: 0.8654\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 274us/sample - loss: 0.4435 - accuracy: 0.8679 - recall: 0.8652 - val_loss: 0.3340 - val_accuracy: 0.8762 - val_recall: 0.8786\n",
      "Epoch 3/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8825 - recall: 0.8810\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 285us/sample - loss: 0.3244 - accuracy: 0.8827 - recall: 0.8814 - val_loss: 0.2785 - val_accuracy: 0.8847 - val_recall: 0.8832\n",
      "Epoch 4/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8912 - recall: 0.8897\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 273us/sample - loss: 0.2951 - accuracy: 0.8904 - recall: 0.8890 - val_loss: 0.2677 - val_accuracy: 0.8906 - val_recall: 0.8913\n",
      "Epoch 5/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.8899 - recall: 0.8897\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2810 - accuracy: 0.8901 - recall: 0.8900 - val_loss: 0.2671 - val_accuracy: 0.8876 - val_recall: 0.8857\n",
      "Epoch 6/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.8952 - recall: 0.8952\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2700 - accuracy: 0.8949 - recall: 0.8948 - val_loss: 0.2709 - val_accuracy: 0.8903 - val_recall: 0.8881\n",
      "Epoch 7/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.8961 - recall: 0.8961\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2706 - accuracy: 0.8966 - recall: 0.8968 - val_loss: 0.2592 - val_accuracy: 0.8928 - val_recall: 0.8922\n",
      "Epoch 8/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.8963 - recall: 0.8958\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2647 - accuracy: 0.8956 - recall: 0.8951 - val_loss: 0.2664 - val_accuracy: 0.8923 - val_recall: 0.8917\n",
      "Epoch 9/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9019 - recall: 0.9011\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 245us/sample - loss: 0.2618 - accuracy: 0.9008 - recall: 0.9000 - val_loss: 0.2585 - val_accuracy: 0.8958 - val_recall: 0.8953\n",
      "Epoch 10/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.9000 - recall: 0.8996\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2560 - accuracy: 0.8987 - recall: 0.8984 - val_loss: 0.2975 - val_accuracy: 0.8956 - val_recall: 0.8955\n",
      "Epoch 11/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.8990 - recall: 0.8990\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 260us/sample - loss: 0.2592 - accuracy: 0.8984 - recall: 0.8983 - val_loss: 0.2457 - val_accuracy: 0.8955 - val_recall: 0.8966\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 115us/sample - loss: 0.2456 - accuracy: 0.8975 - recall: 0.8975\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.9414 - accuracy: 0.8562 - recall: 0.8573\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 289us/sample - loss: 0.9287 - accuracy: 0.8575 - recall: 0.8584 - val_loss: 0.4431 - val_accuracy: 0.8564 - val_recall: 0.8573\n",
      "Epoch 2/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.4436 - accuracy: 0.8760 - recall: 0.8746- ETA: 0s - loss: 0.4957 - accuracy: 0.8682\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 242us/sample - loss: 0.4408 - accuracy: 0.8763 - recall: 0.8752 - val_loss: 0.3138 - val_accuracy: 0.8824 - val_recall: 0.8969\n",
      "Epoch 3/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3532 - accuracy: 0.8838 - recall: 0.8827\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 265us/sample - loss: 0.3532 - accuracy: 0.8836 - recall: 0.8825 - val_loss: 0.2817 - val_accuracy: 0.8879 - val_recall: 0.8846\n",
      "Epoch 4/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8894 - recall: 0.8880\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.3184 - accuracy: 0.8894 - recall: 0.8877 - val_loss: 0.2800 - val_accuracy: 0.8800 - val_recall: 0.8720\n",
      "Epoch 5/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8953 - recall: 0.8928\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 256us/sample - loss: 0.2937 - accuracy: 0.8947 - recall: 0.8920 - val_loss: 0.2702 - val_accuracy: 0.8913 - val_recall: 0.8873\n",
      "Epoch 6/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.8952 - recall: 0.8936\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 249us/sample - loss: 0.2885 - accuracy: 0.8950 - recall: 0.8933 - val_loss: 0.2778 - val_accuracy: 0.8919 - val_recall: 0.8906\n",
      "Epoch 7/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.8980 - recall: 0.8966\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2701 - accuracy: 0.8980 - recall: 0.8966 - val_loss: 0.2613 - val_accuracy: 0.8948 - val_recall: 0.8948\n",
      "Epoch 8/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.8988 - recall: 0.8988\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.2684 - accuracy: 0.8987 - recall: 0.8987 - val_loss: 0.2693 - val_accuracy: 0.8963 - val_recall: 0.8950\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.9001 - recall: 0.8998\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 289us/sample - loss: 0.2596 - accuracy: 0.9001 - recall: 0.8998 - val_loss: 0.2687 - val_accuracy: 0.8955 - val_recall: 0.8944\n",
      "Epoch 10/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.8981 - recall: 0.8967\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 258us/sample - loss: 0.2612 - accuracy: 0.8982 - recall: 0.8968 - val_loss: 0.2613 - val_accuracy: 0.8933 - val_recall: 0.8911\n",
      "Epoch 11/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.9001 - recall: 0.8988\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 335us/sample - loss: 0.2568 - accuracy: 0.9010 - recall: 0.8997 - val_loss: 0.2666 - val_accuracy: 0.8951 - val_recall: 0.8946\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 138us/sample - loss: 0.2611 - accuracy: 0.9007 - recall: 0.9016\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 1.1111 - accuracy: 0.8627 - recall: 0.8600\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 325us/sample - loss: 1.1049 - accuracy: 0.8634 - recall: 0.8606 - val_loss: 0.8482 - val_accuracy: 0.8765 - val_recall: 0.8671\n",
      "Epoch 2/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.6425 - accuracy: 0.8644 - recall: 0.8628\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 244us/sample - loss: 0.6389 - accuracy: 0.8643 - recall: 0.8624 - val_loss: 0.4256 - val_accuracy: 0.8627 - val_recall: 0.8426\n",
      "Epoch 3/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8790 - recall: 0.8801\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.3985 - accuracy: 0.8792 - recall: 0.8802 - val_loss: 0.3080 - val_accuracy: 0.8832 - val_recall: 0.8895\n",
      "Epoch 4/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8902 - recall: 0.8880\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 241us/sample - loss: 0.3397 - accuracy: 0.8904 - recall: 0.8882 - val_loss: 0.2774 - val_accuracy: 0.8899 - val_recall: 0.8893\n",
      "Epoch 5/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8938 - recall: 0.8926\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 243us/sample - loss: 0.2942 - accuracy: 0.8938 - recall: 0.8926 - val_loss: 0.2754 - val_accuracy: 0.8890 - val_recall: 0.8842\n",
      "Epoch 6/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.8966 - recall: 0.8964\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2884 - accuracy: 0.8964 - recall: 0.8964 - val_loss: 0.2854 - val_accuracy: 0.8924 - val_recall: 0.8913\n",
      "Epoch 7/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.8975 - recall: 0.8965\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 305us/sample - loss: 0.2729 - accuracy: 0.8973 - recall: 0.8963 - val_loss: 0.2730 - val_accuracy: 0.8886 - val_recall: 0.8866\n",
      "Epoch 8/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2712 - accuracy: 0.8986 - recall: 0.8985\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 262us/sample - loss: 0.2714 - accuracy: 0.8986 - recall: 0.8985 - val_loss: 0.2763 - val_accuracy: 0.8899 - val_recall: 0.8890\n",
      "Epoch 9/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8979 - recall: 0.8977\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.2883 - accuracy: 0.8980 - recall: 0.8978 - val_loss: 0.2588 - val_accuracy: 0.8943 - val_recall: 0.8964\n",
      "Epoch 10/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9001 - recall: 0.9004\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 267us/sample - loss: 0.2606 - accuracy: 0.9003 - recall: 0.9006 - val_loss: 0.2602 - val_accuracy: 0.8939 - val_recall: 0.8946\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.9006 - recall: 0.9015\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 260us/sample - loss: 0.2617 - accuracy: 0.9007 - recall: 0.9016 - val_loss: 0.2547 - val_accuracy: 0.8949 - val_recall: 0.8960\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 113us/sample - loss: 0.2671 - accuracy: 0.8849 - recall: 0.8852\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 1.0419 - accuracy: 0.8598 - recall: 0.8677\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 316us/sample - loss: 1.0332 - accuracy: 0.8602 - recall: 0.8673 - val_loss: 0.7690 - val_accuracy: 0.8487 - val_recall: 0.8103\n",
      "Epoch 2/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.4781 - accuracy: 0.8656 - recall: 0.8626\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.4730 - accuracy: 0.8663 - recall: 0.8634 - val_loss: 0.3377 - val_accuracy: 0.8775 - val_recall: 0.8750\n",
      "Epoch 3/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8853 - recall: 0.8845\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 268us/sample - loss: 0.3193 - accuracy: 0.8852 - recall: 0.8844 - val_loss: 0.2770 - val_accuracy: 0.8860 - val_recall: 0.8842\n",
      "Epoch 4/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8910 - recall: 0.8918\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 284us/sample - loss: 0.2897 - accuracy: 0.8898 - recall: 0.8906 - val_loss: 0.3101 - val_accuracy: 0.8849 - val_recall: 0.8904\n",
      "Epoch 5/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8910 - recall: 0.8914\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 264us/sample - loss: 0.2834 - accuracy: 0.8901 - recall: 0.8905 - val_loss: 0.3053 - val_accuracy: 0.8895 - val_recall: 0.8900\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8939 - recall: 0.8940\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 265us/sample - loss: 0.2943 - accuracy: 0.8937 - recall: 0.8938 - val_loss: 0.2780 - val_accuracy: 0.8915 - val_recall: 0.8910\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.8968 - recall: 0.8968\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 259us/sample - loss: 0.2640 - accuracy: 0.8971 - recall: 0.8971 - val_loss: 0.2574 - val_accuracy: 0.8927 - val_recall: 0.8913\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2618 - accuracy: 0.8952 - recall: 0.8941\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 253us/sample - loss: 0.2610 - accuracy: 0.8958 - recall: 0.8948 - val_loss: 0.2546 - val_accuracy: 0.8934 - val_recall: 0.8920\n",
      "Epoch 9/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.9002 - recall: 0.8997\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.2600 - accuracy: 0.8999 - recall: 0.8995 - val_loss: 0.2744 - val_accuracy: 0.8895 - val_recall: 0.8887\n",
      "Epoch 10/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.8979 - recall: 0.8984\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 252us/sample - loss: 0.2532 - accuracy: 0.8979 - recall: 0.8983 - val_loss: 0.2559 - val_accuracy: 0.8925 - val_recall: 0.8922\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.8998 - recall: 0.9001\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.2592 - accuracy: 0.8997 - recall: 0.9000 - val_loss: 0.2483 - val_accuracy: 0.8978 - val_recall: 0.8973\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 103us/sample - loss: 0.2464 - accuracy: 0.9042 - recall: 0.9043\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 1.0672 - accuracy: 0.8597 - recall: 0.8658\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 283us/sample - loss: 1.0663 - accuracy: 0.8598 - recall: 0.8665 - val_loss: 0.7982 - val_accuracy: 0.8478 - val_recall: 0.8581\n",
      "Epoch 2/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.5726 - accuracy: 0.8637 - recall: 0.8629\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.5704 - accuracy: 0.8638 - recall: 0.8626 - val_loss: 0.3466 - val_accuracy: 0.8705 - val_recall: 0.8275\n",
      "Epoch 3/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8789 - recall: 0.8789\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 257us/sample - loss: 0.3705 - accuracy: 0.8795 - recall: 0.8795 - val_loss: 0.2951 - val_accuracy: 0.8902 - val_recall: 0.8958\n",
      "Epoch 4/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8863 - recall: 0.8851\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.3193 - accuracy: 0.8865 - recall: 0.8852 - val_loss: 0.2759 - val_accuracy: 0.8877 - val_recall: 0.8864\n",
      "Epoch 5/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8909 - recall: 0.8922\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 251us/sample - loss: 0.2982 - accuracy: 0.8902 - recall: 0.8914 - val_loss: 0.2849 - val_accuracy: 0.8901 - val_recall: 0.8910\n",
      "Epoch 6/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8918 - recall: 0.8931\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2831 - accuracy: 0.8921 - recall: 0.8935 - val_loss: 0.2847 - val_accuracy: 0.8920 - val_recall: 0.8966\n",
      "Epoch 7/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2762 - accuracy: 0.8950 - recall: 0.8964\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2762 - accuracy: 0.8949 - recall: 0.8963 - val_loss: 0.2987 - val_accuracy: 0.8923 - val_recall: 0.8928\n",
      "Epoch 8/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8959 - recall: 0.8971\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.2634 - accuracy: 0.8962 - recall: 0.8977 - val_loss: 0.2698 - val_accuracy: 0.8952 - val_recall: 0.8944\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.8950 - recall: 0.8952\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.2684 - accuracy: 0.8949 - recall: 0.8951 - val_loss: 0.2638 - val_accuracy: 0.8966 - val_recall: 0.8966\n",
      "Epoch 10/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.8970 - recall: 0.8976\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 265us/sample - loss: 0.2617 - accuracy: 0.8964 - recall: 0.8970 - val_loss: 0.2768 - val_accuracy: 0.8969 - val_recall: 0.8973\n",
      "Epoch 11/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.8955 - recall: 0.8966\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 293us/sample - loss: 0.2605 - accuracy: 0.8953 - recall: 0.8963 - val_loss: 0.2630 - val_accuracy: 0.8955 - val_recall: 0.8960\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 111us/sample - loss: 0.2418 - accuracy: 0.9139 - recall: 0.9151\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.8572 - recall: 0.8561\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 246us/sample - loss: 0.6929 - accuracy: 0.8566 - recall: 0.8557 - val_loss: 0.4818 - val_accuracy: 0.8770 - val_recall: 0.8853\n",
      "Epoch 2/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.8821 - recall: 0.8827\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.3828 - accuracy: 0.8828 - recall: 0.8835 - val_loss: 0.3484 - val_accuracy: 0.8888 - val_recall: 0.8864\n",
      "Epoch 3/1000\n",
      "10048/10268 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.8873 - recall: 0.8884\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 238us/sample - loss: 0.3342 - accuracy: 0.8874 - recall: 0.8884 - val_loss: 0.3565 - val_accuracy: 0.8781 - val_recall: 0.8762\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.3128 - accuracy: 0.8910 - recall: 0.8949\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 223us/sample - loss: 0.3129 - accuracy: 0.8913 - recall: 0.8952 - val_loss: 0.2776 - val_accuracy: 0.8910 - val_recall: 0.8901\n",
      "Epoch 5/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.8969 - recall: 0.8963\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2832 - accuracy: 0.8968 - recall: 0.8962 - val_loss: 0.3006 - val_accuracy: 0.8956 - val_recall: 0.8935\n",
      "Epoch 6/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.8969 - recall: 0.8968\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2739 - accuracy: 0.8969 - recall: 0.8968 - val_loss: 0.2711 - val_accuracy: 0.8908 - val_recall: 0.8904\n",
      "Epoch 7/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2725 - accuracy: 0.8983 - recall: 0.8967\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 218us/sample - loss: 0.2725 - accuracy: 0.8983 - recall: 0.8967 - val_loss: 0.2666 - val_accuracy: 0.8937 - val_recall: 0.8926\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.9000 - recall: 0.8991\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 218us/sample - loss: 0.2594 - accuracy: 0.8997 - recall: 0.8987 - val_loss: 0.2539 - val_accuracy: 0.8949 - val_recall: 0.8959\n",
      "Epoch 9/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2626 - accuracy: 0.8991 - recall: 0.8998\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 224us/sample - loss: 0.2623 - accuracy: 0.8994 - recall: 0.9000 - val_loss: 0.2629 - val_accuracy: 0.8983 - val_recall: 0.9006\n",
      "Epoch 10/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.9029 - recall: 0.9030\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 218us/sample - loss: 0.2688 - accuracy: 0.9027 - recall: 0.9028 - val_loss: 0.2525 - val_accuracy: 0.8953 - val_recall: 0.8935\n",
      "Epoch 11/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.9008 - recall: 0.9014\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 218us/sample - loss: 0.2642 - accuracy: 0.9014 - recall: 0.9021 - val_loss: 0.2616 - val_accuracy: 0.8962 - val_recall: 0.9026\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 99us/sample - loss: 0.2625 - accuracy: 0.8991 - recall: 0.9029\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.6798 - accuracy: 0.8643 - recall: 0.8648\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 258us/sample - loss: 0.6744 - accuracy: 0.8651 - recall: 0.8658 - val_loss: 0.3552 - val_accuracy: 0.8827 - val_recall: 0.8868\n",
      "Epoch 2/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8814 - recall: 0.8839\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.3699 - accuracy: 0.8813 - recall: 0.8839 - val_loss: 0.3020 - val_accuracy: 0.8668 - val_recall: 0.8693\n",
      "Epoch 3/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.8868 - recall: 0.8870\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.3232 - accuracy: 0.8867 - recall: 0.8870 - val_loss: 0.2705 - val_accuracy: 0.8859 - val_recall: 0.8881\n",
      "Epoch 4/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8926 - recall: 0.8912\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2926 - accuracy: 0.8929 - recall: 0.8915 - val_loss: 0.2687 - val_accuracy: 0.8890 - val_recall: 0.8840\n",
      "Epoch 5/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2747 - accuracy: 0.8969 - recall: 0.8965\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2740 - accuracy: 0.8972 - recall: 0.8970 - val_loss: 0.2568 - val_accuracy: 0.8922 - val_recall: 0.8924\n",
      "Epoch 6/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.8973 - recall: 0.8972\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2763 - accuracy: 0.8972 - recall: 0.8970 - val_loss: 0.2567 - val_accuracy: 0.8912 - val_recall: 0.8880\n",
      "Epoch 7/1000\n",
      "10016/10268 [============================>.] - ETA: 0s - loss: 0.2619 - accuracy: 0.8979 - recall: 0.8972\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2612 - accuracy: 0.8982 - recall: 0.8974 - val_loss: 0.2592 - val_accuracy: 0.8933 - val_recall: 0.8920\n",
      "Epoch 8/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9007 - recall: 0.9001\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 218us/sample - loss: 0.2622 - accuracy: 0.8999 - recall: 0.8993 - val_loss: 0.2562 - val_accuracy: 0.8921 - val_recall: 0.8915\n",
      "Epoch 9/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.8989 - recall: 0.8970\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2675 - accuracy: 0.8990 - recall: 0.8972 - val_loss: 0.2563 - val_accuracy: 0.8922 - val_recall: 0.8925\n",
      "Epoch 10/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.2585 - accuracy: 0.8995 - recall: 0.8985\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 219us/sample - loss: 0.2582 - accuracy: 0.8995 - recall: 0.8986 - val_loss: 0.2527 - val_accuracy: 0.8964 - val_recall: 0.8938\n",
      "Epoch 11/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.9001 - recall: 0.8993\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2513 - accuracy: 0.9001 - recall: 0.8993 - val_loss: 0.2521 - val_accuracy: 0.8951 - val_recall: 0.8959\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 98us/sample - loss: 0.2464 - accuracy: 0.8931 - recall: 0.8947\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10112/10268 [============================>.] - ETA: 0s - loss: 0.6283 - accuracy: 0.8674 - recall: 0.8633\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 248us/sample - loss: 0.6240 - accuracy: 0.8676 - recall: 0.8636 - val_loss: 0.3118 - val_accuracy: 0.8745 - val_recall: 0.8787\n",
      "Epoch 2/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8875 - recall: 0.8882\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.3496 - accuracy: 0.8877 - recall: 0.8883 - val_loss: 0.2941 - val_accuracy: 0.8690 - val_recall: 0.8666\n",
      "Epoch 3/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.8923 - recall: 0.8884\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.3096 - accuracy: 0.8924 - recall: 0.8885 - val_loss: 0.2683 - val_accuracy: 0.8890 - val_recall: 0.8893\n",
      "Epoch 4/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.8958 - recall: 0.8961\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2842 - accuracy: 0.8958 - recall: 0.8962 - val_loss: 0.2948 - val_accuracy: 0.8910 - val_recall: 0.8926\n",
      "Epoch 5/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8989 - recall: 0.8984\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2808 - accuracy: 0.8987 - recall: 0.8981 - val_loss: 0.2630 - val_accuracy: 0.8874 - val_recall: 0.8897\n",
      "Epoch 6/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2637 - accuracy: 0.8990 - recall: 0.9005\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2642 - accuracy: 0.8992 - recall: 0.9007 - val_loss: 0.2562 - val_accuracy: 0.8949 - val_recall: 0.8942\n",
      "Epoch 7/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.9006 - recall: 0.9003\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2601 - accuracy: 0.9007 - recall: 0.9004 - val_loss: 0.2529 - val_accuracy: 0.8919 - val_recall: 0.8900\n",
      "Epoch 8/1000\n",
      "10080/10268 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.9020 - recall: 0.9019\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 219us/sample - loss: 0.2609 - accuracy: 0.9028 - recall: 0.9027 - val_loss: 0.2644 - val_accuracy: 0.8904 - val_recall: 0.8911\n",
      "Epoch 9/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.9002 - recall: 0.9005\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 220us/sample - loss: 0.2529 - accuracy: 0.9000 - recall: 0.9005 - val_loss: 0.2662 - val_accuracy: 0.8922 - val_recall: 0.8922\n",
      "Epoch 10/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.9036 - recall: 0.9041\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2534 - accuracy: 0.9036 - recall: 0.9041 - val_loss: 0.2728 - val_accuracy: 0.8976 - val_recall: 0.8980\n",
      "Epoch 11/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.9049 - recall: 0.9051\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2507 - accuracy: 0.9050 - recall: 0.9052 - val_loss: 0.2754 - val_accuracy: 0.8984 - val_recall: 0.8988\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 96us/sample - loss: 0.2820 - accuracy: 0.8960 - recall: 0.8968\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.7966 - accuracy: 0.8540 - recall: 0.8498\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 250us/sample - loss: 0.7920 - accuracy: 0.8541 - recall: 0.8498 - val_loss: 0.4453 - val_accuracy: 0.8755 - val_recall: 0.8712\n",
      "Epoch 2/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.4146 - accuracy: 0.8750 - recall: 0.8732\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.4180 - accuracy: 0.8749 - recall: 0.8731 - val_loss: 0.4019 - val_accuracy: 0.8879 - val_recall: 0.8817\n",
      "Epoch 3/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8874 - recall: 0.8852\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 220us/sample - loss: 0.3334 - accuracy: 0.8878 - recall: 0.8856 - val_loss: 0.2785 - val_accuracy: 0.8936 - val_recall: 0.8907\n",
      "Epoch 4/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.8936 - recall: 0.8949\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 220us/sample - loss: 0.2829 - accuracy: 0.8931 - recall: 0.8944 - val_loss: 0.2884 - val_accuracy: 0.8887 - val_recall: 0.8926\n",
      "Epoch 5/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.8956 - recall: 0.8971\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2754 - accuracy: 0.8957 - recall: 0.8972 - val_loss: 0.3213 - val_accuracy: 0.8930 - val_recall: 0.8941\n",
      "Epoch 6/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2730 - accuracy: 0.8960 - recall: 0.8960\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2737 - accuracy: 0.8958 - recall: 0.8958 - val_loss: 0.2566 - val_accuracy: 0.8978 - val_recall: 0.8957\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.8978 - recall: 0.8978\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2686 - accuracy: 0.8978 - recall: 0.8978 - val_loss: 0.2591 - val_accuracy: 0.8946 - val_recall: 0.8930\n",
      "Epoch 8/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.8983 - recall: 0.8979\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2541 - accuracy: 0.8984 - recall: 0.8980 - val_loss: 0.2564 - val_accuracy: 0.8942 - val_recall: 0.8915\n",
      "Epoch 9/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2556 - accuracy: 0.9007 - recall: 0.9007\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2565 - accuracy: 0.9000 - recall: 0.9000 - val_loss: 0.2531 - val_accuracy: 0.8920 - val_recall: 0.8915\n",
      "Epoch 10/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.2563 - accuracy: 0.8987 - recall: 0.8977\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2557 - accuracy: 0.8993 - recall: 0.8982 - val_loss: 0.2517 - val_accuracy: 0.8939 - val_recall: 0.8927\n",
      "Epoch 11/1000\n",
      "10144/10268 [============================>.] - ETA: 0s - loss: 0.2624 - accuracy: 0.9007 - recall: 0.8999\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 219us/sample - loss: 0.2617 - accuracy: 0.9010 - recall: 0.9002 - val_loss: 0.2545 - val_accuracy: 0.8974 - val_recall: 0.8977\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 97us/sample - loss: 0.2621 - accuracy: 0.9030 - recall: 0.9039\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10268 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.7569 - accuracy: 0.8531 - recall: 0.8562\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 3s 247us/sample - loss: 0.7538 - accuracy: 0.8535 - recall: 0.8564 - val_loss: 0.4450 - val_accuracy: 0.8594 - val_recall: 0.8441\n",
      "Epoch 2/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.4433 - accuracy: 0.8717 - recall: 0.8687\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.4444 - accuracy: 0.8720 - recall: 0.8696 - val_loss: 0.3676 - val_accuracy: 0.8513 - val_recall: 0.7727\n",
      "Epoch 3/1000\n",
      "10208/10268 [============================>.] - ETA: 0s - loss: 0.3404 - accuracy: 0.8844 - recall: 0.8766\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.3406 - accuracy: 0.8843 - recall: 0.8764 - val_loss: 0.2901 - val_accuracy: 0.8889 - val_recall: 0.8855\n",
      "Epoch 4/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8863 - recall: 0.8831\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 218us/sample - loss: 0.3261 - accuracy: 0.8862 - recall: 0.8830 - val_loss: 0.2669 - val_accuracy: 0.8868 - val_recall: 0.8911\n",
      "Epoch 5/1000\n",
      " 9952/10268 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8830 - recall: 0.8820\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 214us/sample - loss: 0.3007 - accuracy: 0.8844 - recall: 0.8836 - val_loss: 0.2614 - val_accuracy: 0.8953 - val_recall: 0.8972\n",
      "Epoch 6/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.8928 - recall: 0.8915\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2808 - accuracy: 0.8926 - recall: 0.8913 - val_loss: 0.2565 - val_accuracy: 0.8940 - val_recall: 0.8949\n",
      "Epoch 7/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2735 - accuracy: 0.8930 - recall: 0.8914\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 216us/sample - loss: 0.2733 - accuracy: 0.8931 - recall: 0.8915 - val_loss: 0.2577 - val_accuracy: 0.8901 - val_recall: 0.8866\n",
      "Epoch 8/1000\n",
      "10176/10268 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.8981 - recall: 0.8968\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 218us/sample - loss: 0.2682 - accuracy: 0.8984 - recall: 0.8971 - val_loss: 0.2597 - val_accuracy: 0.8943 - val_recall: 0.8937\n",
      "Epoch 9/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2551 - accuracy: 0.8967 - recall: 0.8946\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 217us/sample - loss: 0.2552 - accuracy: 0.8966 - recall: 0.8945 - val_loss: 0.2578 - val_accuracy: 0.8905 - val_recall: 0.8910\n",
      "Epoch 10/1000\n",
      "10240/10268 [============================>.] - ETA: 0s - loss: 0.2556 - accuracy: 0.8966 - recall: 0.8958\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2553 - accuracy: 0.8968 - recall: 0.8960 - val_loss: 0.2460 - val_accuracy: 0.8963 - val_recall: 0.8931\n",
      "Epoch 11/1000\n",
      " 9984/10268 [============================>.] - ETA: 0s - loss: 0.2553 - accuracy: 0.8972 - recall: 0.8937\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "10268/10268 [==============================] - 2s 215us/sample - loss: 0.2544 - accuracy: 0.8978 - recall: 0.8944 - val_loss: 0.2475 - val_accuracy: 0.8974 - val_recall: 0.8950\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567/2567 [==============================] - 0s 98us/sample - loss: 0.2246 - accuracy: 0.9118 - recall: 0.9097\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000002CA634CEB00>, as the constructor either does not set or modifies parameter dropout3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[1;32m--> 736\u001b[1;33m                 **self.best_params_))\n\u001b[0m\u001b[0;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     80\u001b[0m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0;32m     81\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                                (estimator, name))\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000002CA634CEB00>, as the constructor either does not set or modifies parameter dropout3"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs.fit(X_train,y_train,verbose=1,\n",
    "        validation_data=(x_validation, y_validation),callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 1000,\n",
       " 'dropout3': 0.4,\n",
       " 'dropout2': 0.0,\n",
       " 'dropout1': 0.0,\n",
       " 'dense_dims3': 512,\n",
       " 'dense_dims2': 128,\n",
       " 'dense_dims1': 512}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the best model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_model(optimizer='adam',\n",
    "     dropout3 = 0.4,\n",
    "     dropout2 = 0.0,\n",
    "     dropout1 = 0.0,\n",
    "     dense_dims3 = 512,\n",
    "     dense_dims2 = 128,\n",
    "     dense_dims1 = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12835 samples, validate on 5502 samples\n",
      "Epoch 1/1000\n",
      "12672/12835 [============================>.] - ETA: 0s - loss: 0.6020 - accuracy: 0.8718 - recall: 0.8707\n",
      "Epoch 00001: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 255us/sample - loss: 0.5974 - accuracy: 0.8723 - recall: 0.8716 - val_loss: 0.3270 - val_accuracy: 0.8866 - val_recall: 0.8867\n",
      "Epoch 2/1000\n",
      "12768/12835 [============================>.] - ETA: 0s - loss: 0.3389 - accuracy: 0.8892 - recall: 0.8909\n",
      "Epoch 00002: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 225us/sample - loss: 0.3390 - accuracy: 0.8893 - recall: 0.8912 - val_loss: 0.2636 - val_accuracy: 0.8930 - val_recall: 0.8924\n",
      "Epoch 3/1000\n",
      "12608/12835 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.8974 - recall: 0.8971\n",
      "Epoch 00003: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 217us/sample - loss: 0.2853 - accuracy: 0.8970 - recall: 0.8969 - val_loss: 0.2609 - val_accuracy: 0.8889 - val_recall: 0.8880\n",
      "Epoch 4/1000\n",
      "12672/12835 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.8966 - recall: 0.8951\n",
      "Epoch 00004: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 209us/sample - loss: 0.2709 - accuracy: 0.8964 - recall: 0.8951 - val_loss: 0.2551 - val_accuracy: 0.8941 - val_recall: 0.8941\n",
      "Epoch 5/1000\n",
      "12608/12835 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.8985 - recall: 0.8986\n",
      "Epoch 00005: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 217us/sample - loss: 0.2578 - accuracy: 0.8989 - recall: 0.8993 - val_loss: 0.2677 - val_accuracy: 0.8906 - val_recall: 0.8908\n",
      "Epoch 6/1000\n",
      "12736/12835 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9030 - recall: 0.9026\n",
      "Epoch 00006: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 223us/sample - loss: 0.2580 - accuracy: 0.9026 - recall: 0.9025 - val_loss: 0.2602 - val_accuracy: 0.9008 - val_recall: 0.9055\n",
      "Epoch 7/1000\n",
      "12544/12835 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.9025 - recall: 0.9023\n",
      "Epoch 00007: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 212us/sample - loss: 0.2558 - accuracy: 0.9018 - recall: 0.9011 - val_loss: 0.2513 - val_accuracy: 0.8929 - val_recall: 0.8928\n",
      "Epoch 8/1000\n",
      "12608/12835 [============================>.] - ETA: 0s - loss: 0.2545 - accuracy: 0.9029 - recall: 0.9034\n",
      "Epoch 00008: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 220us/sample - loss: 0.2543 - accuracy: 0.9030 - recall: 0.9029 - val_loss: 0.2431 - val_accuracy: 0.8989 - val_recall: 0.8986\n",
      "Epoch 9/1000\n",
      "12768/12835 [============================>.] - ETA: 0s - loss: 0.2473 - accuracy: 0.9044 - recall: 0.9044\n",
      "Epoch 00009: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 213us/sample - loss: 0.2469 - accuracy: 0.9048 - recall: 0.9049 - val_loss: 0.2634 - val_accuracy: 0.9008 - val_recall: 0.9017\n",
      "Epoch 10/1000\n",
      "12576/12835 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.9010 - recall: 0.9020\n",
      "Epoch 00010: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 232us/sample - loss: 0.2747 - accuracy: 0.9009 - recall: 0.9013 - val_loss: 0.2523 - val_accuracy: 0.8942 - val_recall: 0.8942\n",
      "Epoch 11/1000\n",
      "12832/12835 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.8998 - recall: 0.8997\n",
      "Epoch 00011: val_recall did not improve from 0.94130\n",
      "12835/12835 [==============================] - 3s 227us/sample - loss: 0.2503 - accuracy: 0.8998 - recall: 0.9000 - val_loss: 0.2618 - val_accuracy: 0.8952 - val_recall: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cec26749e8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X_train,y_train,verbose=1,\n",
    "        validation_data=(x_validation, y_validation),callbacks = callbacks, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "5502/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 143us/sample - loss: 0.2340 - accuracy: 0.8952 - recall: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2617971995762156, 0.8952199, 0.89551836]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The best model from grid search gives a 89% recall on train and evaluation, there was a model with 96% recall but I have to do some research to fix the issue with randomsearchcv and keras.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "455px",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
